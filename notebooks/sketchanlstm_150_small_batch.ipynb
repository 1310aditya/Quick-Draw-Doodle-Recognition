{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "522ce330-d4f9-4fbe-9684-4b65fd684cca",
    "_uuid": "174484daa5f084ce4970f5048d3e05d2c4429787"
   },
   "source": [
    "# Overview\n",
    "The notebook is modified from one that was made for the [Quick, Draw Dataset](https://www.kaggle.com/google/tinyquickdraw), it would actually be interesting to see how beneficial a transfer learning approach using that data as a starting point could be.\n",
    "\n",
    "## This Notebook\n",
    "The notebook takes and preprocesses the data from the QuickDraw Competition step (strokes) and trains an LSTM. The outcome variable (y) is always the same (category). The stroke-based LSTM. The model takes the stroke data and 'preprocesses' it a bit using 1D convolutions and then uses two stacked LSTMs followed by two dense layers to make the classification. The model can be thought to 'read' the drawing stroke by stroke.\n",
    "\n",
    "## Fun Models\n",
    "\n",
    "After the classification models, we try to build a few models to understand what the LSTM actually does. Here we experiment step by step to see how the prediction changes with each stop\n",
    "\n",
    "### Next Steps\n",
    "The next steps could be\n",
    "- use more data to train\n",
    "- include the country code (different countries draw different things, different ways)\n",
    "- more complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8ccded02a5f2c4a9d9ee2f7688114bcd2e1f11a"
   },
   "source": [
    "### Model Parameters\n",
    "Here we keep track of the relevant parameters for the data preprocessing, model construction and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "8b08fbab2000a563b388f126eac74362641e497c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "STROKE_COUNT = 196\n",
    "TRAIN_SAMPLES = 1750\n",
    "VALID_SAMPLES = 300\n",
    "TEST_SAMPLES = 300\n",
    "NUM_CLASSES = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(69)\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from glob import glob\n",
    "import gc\n",
    "gc.enable()\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "base_dir = os.path.join('..', 'input')\n",
    "test_path = os.path.join(base_dir, 'test_simplified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "7acacf8e960084782425ef1a1a3fd532a240ad48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "ALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\n",
    "COL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
    "\n",
    "def _stack_it(raw_strokes):\n",
    "    \"\"\"preprocess the string and make \n",
    "    a standard Nx3 stroke vector\"\"\"\n",
    "    stroke_vec = literal_eval(raw_strokes) # string->list\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,i)  \n",
    "     for i,(x,y) in enumerate(stroke_vec) \n",
    "     for xi,yi in zip(x,y)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "    # pad the strokes with zeros\n",
    "    x = pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "                         maxlen=STROKE_COUNT, \n",
    "                         padding='post').swapaxes(0, 1)\n",
    "    return x\n",
    "\n",
    "def read_batch(samples=5, \n",
    "               start_row=0,\n",
    "               max_rows = 1000):\n",
    "    \"\"\"\n",
    "    load and process the csv files\n",
    "    this function is horribly inefficient but simple\n",
    "    \"\"\"\n",
    "    out_df_list = []\n",
    "    for c_path in ALL_TRAIN_PATHS[:NUM_CLASSES]:\n",
    "        c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n",
    "        c_df.columns=COL_NAMES\n",
    "        out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n",
    "    full_df = pd.concat(out_df_list)\n",
    "    full_df['drawing'] = full_df['drawing'].\\\n",
    "        map(_stack_it)\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ec1854b21b36cd2fd7f7d0717aaa8da32506a6a"
   },
   "source": [
    "# Reading and Parsing\n",
    "Since it is too much data (23GB) to read in at once, we just take a portion of it for training, validation and hold-out testing. This should give us an idea about how well the model works, but leaves lots of room for improvement later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words 150 => The Great Wall of China, alarm clock, angel, animal migration, ant, anvil, apple, asparagus, axe, banana, barn, baseball, bat, bathtub, beach, bear, bicycle, birthday cake, blackberry, blueberry, book, boomerang, bowtie, bread, bridge, broccoli, bus, bush, butterfly, cake, calendar, campfire, canoe, cat, ceiling fan, cell phone, chandelier, church, circle, clarinet, computer, cookie, crown, cup, diving board, dog, dolphin, donut, door, dresser, drill, elbow, elephant, eyeglasses, face, fan, fireplace, flip flops, foot, garden, golf club, grapes, grass, hamburger, hammer, headphones, helicopter, hockey puck, hospital, hot air balloon, hot dog, hourglass, house, house plant, hurricane, ice cream, jacket, jail, key, lantern, map, marker, megaphone, mermaid, monkey, mosquito, motorbike, mountain, mushroom, nose, ocean, octopus, onion, paper clip, parrot, passport, peanut, pineapple, pliers, police car, pool, popsicle, potato, purse, rake, river, roller coaster, sandwich, sea turtle, see saw, shark, sink, skateboard, sleeping bag, snail, soccer ball, sock, spider, spoon, spreadsheet, square, squiggle, squirrel, stereo, stethoscope, stove, suitcase, swing set, t-shirt, television, tent, toe, tooth, toothbrush, toothpaste, traffic light, tree, triangle, trumpet, van, violin, washing machine, watermelon, waterslide, wheel, windmill, wine glass, yoga, zebra, zigzag\n"
     ]
    }
   ],
   "source": [
    "train_args = dict(samples=TRAIN_SAMPLES, \n",
    "                  start_row=0, \n",
    "                  max_rows=int(TRAIN_SAMPLES*1.5))\n",
    "valid_args = dict(samples=VALID_SAMPLES, \n",
    "                  start_row=train_args['max_rows']+1, \n",
    "                  max_rows=VALID_SAMPLES+25)\n",
    "test_args = dict(samples=TEST_SAMPLES, \n",
    "                 start_row=valid_args['max_rows']+train_args['max_rows']+1, \n",
    "                 max_rows=TEST_SAMPLES+25)\n",
    "train_df = read_batch(**train_args)\n",
    "valid_df = read_batch(**valid_args)\n",
    "test_df = read_batch(**test_args)\n",
    "word_encoder = LabelEncoder()\n",
    "word_encoder.fit(train_df['word'])\n",
    "print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d29237e-ece3-4dfd-9095-475296f4a608",
    "_uuid": "8bae16a4973a215861fbb536a602c4f5abf3b4bf"
   },
   "source": [
    "# Stroke-based Classification\n",
    "Here we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "ff5ddced-d77e-473f-899d-82cf11ad2bd9",
    "_uuid": "409468f1d5abd17b819482473a4f354a61f8d7ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262500, 196, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_Xy(in_df):\n",
    "    X = np.stack(in_df['drawing'], 0)\n",
    "    y = to_categorical(word_encoder.transform(in_df['word'].values))\n",
    "    return X, y\n",
    "train_X, train_y = get_Xy(train_df)\n",
    "valid_X, valid_y = get_Xy(valid_df)\n",
    "test_X, test_y = get_Xy(test_df)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "56240ed9-42b0-4f62-b3d1-f92017f04e30",
    "_uuid": "5cc79204a0a1da048d1d58ba8dfdafd0af3ebcb8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
    "# rand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\n",
    "# for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
    "#     test_arr = train_X[c_id]\n",
    "#     test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "#     lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "#     for i in np.unique(lab_idx):\n",
    "#         c_ax.plot(test_arr[lab_idx==i,0], \n",
    "#                 np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
    "#     c_ax.axis('off')\n",
    "#     c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e1d5bba-0fb4-432c-bd0b-ad69be0ef9ac",
    "_uuid": "b4a087a17798c2ec8eb520bc916bcad38d4ebff2",
    "collapsed": true
   },
   "source": [
    "# LSTM to Parse Strokes\n",
    "The model suggeted from the tutorial is\n",
    "\n",
    "![Suggested Model](https://www.tensorflow.org/versions/master/images/quickdraw_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "f0f65d20e07c0dbdc194e566f5b96c5cb14d03b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout, MaxPool1D\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "d65490e7-302e-4232-afe7-4e9499010e31",
    "_uuid": "ba9d55554ba9e4177df5f0645ca1e0f5e4393ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, None, 3)           12        \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 256)         98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, None, 128)         197632    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               76950     \n",
      "=================================================================\n",
      "Total params: 967,074\n",
      "Trainable params: 967,068\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if len(get_available_gpus())>0:\n",
    "    # https://twitter.com/fchollet/status/918170264608817152?lang=en\n",
    "    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances\n",
    "stroke_read_model = Sequential()\n",
    "stroke_read_model.add(BatchNormalization(input_shape = (None,)+train_X.shape[2:]))\n",
    "# filter count and length are taken from the script https://github.com/tensorflow/models/blob/master/tutorials/rnn/quickdraw/train_model.py\n",
    "stroke_read_model.add(Conv1D(128, (5,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(MaxPool1D(pool_size=3, strides=2))\n",
    "stroke_read_model.add(Conv1D(256, (3,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(Conv1D(256, (3,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(Conv1D(256, (3,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(MaxPool1D(pool_size=3, strides=2))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = True))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = False))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Dense(512))\n",
    "stroke_read_model.add(Dropout(0.4))\n",
    "stroke_read_model.add(Dense(len(word_encoder.classes_), activation = 'softmax'))\n",
    "#adam = optimizers.Adam(lr=0.001)\n",
    "stroke_read_model.compile(optimizer = 'adam', \n",
    "                          loss = 'categorical_crossentropy', \n",
    "                          metrics = ['categorical_accuracy', top_3_accuracy])\n",
    "stroke_read_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "2a549512-a9d9-4afd-b748-3e1c3296e193",
    "_uuid": "5fda10b30c47a8cf6ea822ed0a4a1d7cd2c81195",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=3, \n",
    "                                   verbose=1, mode='auto', cooldown=3, min_lr=0.000005)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=10) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "ecc9068b26bf6022138a7970f1a6612f2012e74a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import Callback\n",
    "# class OutputClearNEpoch(Callback):\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         current = logs.get(self.monitor)\n",
    "#         if epoch % 5 == 0:\n",
    "#             clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "825b3af8-9451-487b-a1e1-538f2f1489e1",
    "_uuid": "ed2fc26af74aed1a93bbc253d61b72db5a81f5cc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 262500 samples, validate on 45000 samples\n",
      "Epoch 1/150\n",
      "262500/262500 [==============================] - 113s 432us/step - loss: 4.7622 - categorical_accuracy: 0.0194 - top_3_accuracy: 0.0535 - val_loss: 4.6364 - val_categorical_accuracy: 0.0280 - val_top_3_accuracy: 0.0766\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.63645, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 2/150\n",
      "262500/262500 [==============================] - 108s 413us/step - loss: 4.6360 - categorical_accuracy: 0.0260 - top_3_accuracy: 0.0702 - val_loss: 4.6835 - val_categorical_accuracy: 0.0229 - val_top_3_accuracy: 0.0626\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 4.63645\n",
      "Epoch 3/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 4.6196 - categorical_accuracy: 0.0272 - top_3_accuracy: 0.0729 - val_loss: 4.5916 - val_categorical_accuracy: 0.0312 - val_top_3_accuracy: 0.0823\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.63645 to 4.59162, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 4/150\n",
      "262500/262500 [==============================] - 108s 413us/step - loss: 4.5098 - categorical_accuracy: 0.0364 - top_3_accuracy: 0.0963 - val_loss: 4.4142 - val_categorical_accuracy: 0.0442 - val_top_3_accuracy: 0.1160\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.59162 to 4.41417, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 5/150\n",
      "262500/262500 [==============================] - 108s 413us/step - loss: 4.3324 - categorical_accuracy: 0.0537 - top_3_accuracy: 0.1350 - val_loss: 4.2330 - val_categorical_accuracy: 0.0660 - val_top_3_accuracy: 0.1596\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.41417 to 4.23300, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 6/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 4.1734 - categorical_accuracy: 0.0733 - top_3_accuracy: 0.1736 - val_loss: 3.9658 - val_categorical_accuracy: 0.1076 - val_top_3_accuracy: 0.2356\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.23300 to 3.96577, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 7/150\n",
      "262500/262500 [==============================] - 108s 413us/step - loss: 3.8885 - categorical_accuracy: 0.1153 - top_3_accuracy: 0.2494 - val_loss: 3.5874 - val_categorical_accuracy: 0.1638 - val_top_3_accuracy: 0.3281\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.96577 to 3.58742, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 8/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 3.5028 - categorical_accuracy: 0.1713 - top_3_accuracy: 0.3451 - val_loss: 3.2810 - val_categorical_accuracy: 0.2146 - val_top_3_accuracy: 0.4081\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.58742 to 3.28099, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 9/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 3.1570 - categorical_accuracy: 0.2312 - top_3_accuracy: 0.4358 - val_loss: 2.9278 - val_categorical_accuracy: 0.2817 - val_top_3_accuracy: 0.4948\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.28099 to 2.92781, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 10/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 2.9095 - categorical_accuracy: 0.2820 - top_3_accuracy: 0.5005 - val_loss: 2.6495 - val_categorical_accuracy: 0.3411 - val_top_3_accuracy: 0.5645\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.92781 to 2.64948, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 11/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 2.6678 - categorical_accuracy: 0.3363 - top_3_accuracy: 0.5631 - val_loss: 2.4910 - val_categorical_accuracy: 0.3764 - val_top_3_accuracy: 0.5982\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.64948 to 2.49102, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 12/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 2.4821 - categorical_accuracy: 0.3786 - top_3_accuracy: 0.6063 - val_loss: 2.4251 - val_categorical_accuracy: 0.3918 - val_top_3_accuracy: 0.6171\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.49102 to 2.42508, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 13/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 2.3412 - categorical_accuracy: 0.4129 - top_3_accuracy: 0.6394 - val_loss: 2.1090 - val_categorical_accuracy: 0.4658 - val_top_3_accuracy: 0.6904\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.42508 to 2.10901, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 14/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 2.2161 - categorical_accuracy: 0.4419 - top_3_accuracy: 0.6670 - val_loss: 2.0310 - val_categorical_accuracy: 0.4851 - val_top_3_accuracy: 0.7059\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.10901 to 2.03098, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 15/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 2.1103 - categorical_accuracy: 0.4677 - top_3_accuracy: 0.6901 - val_loss: 1.9678 - val_categorical_accuracy: 0.4970 - val_top_3_accuracy: 0.7192\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.03098 to 1.96777, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 16/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 2.0162 - categorical_accuracy: 0.4894 - top_3_accuracy: 0.7091 - val_loss: 1.9726 - val_categorical_accuracy: 0.5001 - val_top_3_accuracy: 0.7179\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.96777\n",
      "Epoch 17/150\n",
      "262500/262500 [==============================] - 108s 413us/step - loss: 1.9401 - categorical_accuracy: 0.5086 - top_3_accuracy: 0.7247 - val_loss: 1.7516 - val_categorical_accuracy: 0.5533 - val_top_3_accuracy: 0.7620\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.96777 to 1.75156, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 18/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.8705 - categorical_accuracy: 0.5255 - top_3_accuracy: 0.7395 - val_loss: 1.7121 - val_categorical_accuracy: 0.5624 - val_top_3_accuracy: 0.7691\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.75156 to 1.71208, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 19/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.8180 - categorical_accuracy: 0.5387 - top_3_accuracy: 0.7485 - val_loss: 1.7378 - val_categorical_accuracy: 0.5574 - val_top_3_accuracy: 0.7630\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.71208\n",
      "Epoch 20/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.7553 - categorical_accuracy: 0.5540 - top_3_accuracy: 0.7607 - val_loss: 1.6287 - val_categorical_accuracy: 0.5847 - val_top_3_accuracy: 0.7850\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.71208 to 1.62869, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 21/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.7049 - categorical_accuracy: 0.5668 - top_3_accuracy: 0.7706 - val_loss: 1.5869 - val_categorical_accuracy: 0.5928 - val_top_3_accuracy: 0.7914\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.62869 to 1.58689, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 22/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.6576 - categorical_accuracy: 0.5792 - top_3_accuracy: 0.7785 - val_loss: 1.5891 - val_categorical_accuracy: 0.5942 - val_top_3_accuracy: 0.7908\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.58689\n",
      "Epoch 23/150\n",
      "262500/262500 [==============================] - 108s 413us/step - loss: 1.6133 - categorical_accuracy: 0.5897 - top_3_accuracy: 0.7860 - val_loss: 1.6272 - val_categorical_accuracy: 0.5851 - val_top_3_accuracy: 0.7824\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.58689\n",
      "Epoch 24/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.5893 - categorical_accuracy: 0.5958 - top_3_accuracy: 0.7902 - val_loss: 1.4541 - val_categorical_accuracy: 0.6283 - val_top_3_accuracy: 0.8147\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.58689 to 1.45409, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 25/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.5441 - categorical_accuracy: 0.6067 - top_3_accuracy: 0.7982 - val_loss: 1.4199 - val_categorical_accuracy: 0.6372 - val_top_3_accuracy: 0.8204\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.45409 to 1.41995, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 26/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.5164 - categorical_accuracy: 0.6133 - top_3_accuracy: 0.8029 - val_loss: 1.3705 - val_categorical_accuracy: 0.6490 - val_top_3_accuracy: 0.8294\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.41995 to 1.37051, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 27/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.4893 - categorical_accuracy: 0.6205 - top_3_accuracy: 0.8087 - val_loss: 1.3846 - val_categorical_accuracy: 0.6455 - val_top_3_accuracy: 0.8269\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.37051\n",
      "Epoch 28/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.4625 - categorical_accuracy: 0.6269 - top_3_accuracy: 0.8125 - val_loss: 1.3362 - val_categorical_accuracy: 0.6594 - val_top_3_accuracy: 0.8345\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.37051 to 1.33620, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 29/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.4364 - categorical_accuracy: 0.6345 - top_3_accuracy: 0.8167 - val_loss: 1.3677 - val_categorical_accuracy: 0.6503 - val_top_3_accuracy: 0.8308\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.33620\n",
      "Epoch 30/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.4144 - categorical_accuracy: 0.6404 - top_3_accuracy: 0.8207 - val_loss: 1.3034 - val_categorical_accuracy: 0.6676 - val_top_3_accuracy: 0.8383\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.33620 to 1.30341, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 31/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.3838 - categorical_accuracy: 0.6466 - top_3_accuracy: 0.8255 - val_loss: 1.3197 - val_categorical_accuracy: 0.6625 - val_top_3_accuracy: 0.8358\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.30341\n",
      "Epoch 32/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.3686 - categorical_accuracy: 0.6505 - top_3_accuracy: 0.8275 - val_loss: 1.2831 - val_categorical_accuracy: 0.6722 - val_top_3_accuracy: 0.8429\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.30341 to 1.28311, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 33/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.3473 - categorical_accuracy: 0.6567 - top_3_accuracy: 0.8309 - val_loss: 1.2354 - val_categorical_accuracy: 0.6844 - val_top_3_accuracy: 0.8502\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.28311 to 1.23535, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 34/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.3330 - categorical_accuracy: 0.6591 - top_3_accuracy: 0.8335 - val_loss: 1.2954 - val_categorical_accuracy: 0.6677 - val_top_3_accuracy: 0.8405\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.23535\n",
      "Epoch 35/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.3180 - categorical_accuracy: 0.6637 - top_3_accuracy: 0.8371 - val_loss: 1.2735 - val_categorical_accuracy: 0.6735 - val_top_3_accuracy: 0.8448\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.23535\n",
      "Epoch 36/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.2935 - categorical_accuracy: 0.6692 - top_3_accuracy: 0.8400 - val_loss: 1.2203 - val_categorical_accuracy: 0.6863 - val_top_3_accuracy: 0.8533\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.23535 to 1.22026, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 37/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.2794 - categorical_accuracy: 0.6721 - top_3_accuracy: 0.8427 - val_loss: 1.2706 - val_categorical_accuracy: 0.6758 - val_top_3_accuracy: 0.8437\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.22026\n",
      "Epoch 38/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.2710 - categorical_accuracy: 0.6744 - top_3_accuracy: 0.8440 - val_loss: 1.1760 - val_categorical_accuracy: 0.6991 - val_top_3_accuracy: 0.8593\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.22026 to 1.17604, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 39/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.2474 - categorical_accuracy: 0.6808 - top_3_accuracy: 0.8479 - val_loss: 1.1921 - val_categorical_accuracy: 0.6962 - val_top_3_accuracy: 0.8570\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.17604\n",
      "Epoch 40/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.2369 - categorical_accuracy: 0.6836 - top_3_accuracy: 0.8489 - val_loss: 1.2492 - val_categorical_accuracy: 0.6836 - val_top_3_accuracy: 0.8477\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.17604\n",
      "Epoch 41/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.2291 - categorical_accuracy: 0.6847 - top_3_accuracy: 0.8502 - val_loss: 1.1850 - val_categorical_accuracy: 0.6977 - val_top_3_accuracy: 0.8585\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.17604\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Epoch 42/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.1841 - categorical_accuracy: 0.6961 - top_3_accuracy: 0.8577 - val_loss: 1.1476 - val_categorical_accuracy: 0.7066 - val_top_3_accuracy: 0.8634\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.17604 to 1.14762, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 43/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 1.1643 - categorical_accuracy: 0.7009 - top_3_accuracy: 0.8607 - val_loss: 1.1549 - val_categorical_accuracy: 0.7056 - val_top_3_accuracy: 0.8618\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.14762\n",
      "Epoch 44/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.1670 - categorical_accuracy: 0.7010 - top_3_accuracy: 0.8592 - val_loss: 1.1150 - val_categorical_accuracy: 0.7151 - val_top_3_accuracy: 0.8680\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.14762 to 1.11505, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 45/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.1503 - categorical_accuracy: 0.7042 - top_3_accuracy: 0.8622 - val_loss: 1.1306 - val_categorical_accuracy: 0.7132 - val_top_3_accuracy: 0.8663\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.11505\n",
      "Epoch 46/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.1471 - categorical_accuracy: 0.7051 - top_3_accuracy: 0.8634 - val_loss: 1.1007 - val_categorical_accuracy: 0.7193 - val_top_3_accuracy: 0.8706\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.11505 to 1.10065, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 47/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.1400 - categorical_accuracy: 0.7075 - top_3_accuracy: 0.8638 - val_loss: 1.1138 - val_categorical_accuracy: 0.7157 - val_top_3_accuracy: 0.8684\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.10065\n",
      "Epoch 48/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 1.1320 - categorical_accuracy: 0.7087 - top_3_accuracy: 0.8661 - val_loss: 1.1107 - val_categorical_accuracy: 0.7152 - val_top_3_accuracy: 0.8688\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.10065\n",
      "Epoch 49/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.1286 - categorical_accuracy: 0.7103 - top_3_accuracy: 0.8665 - val_loss: 1.1088 - val_categorical_accuracy: 0.7165 - val_top_3_accuracy: 0.8673\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.10065\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "Epoch 50/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.1007 - categorical_accuracy: 0.7172 - top_3_accuracy: 0.8697 - val_loss: 1.0900 - val_categorical_accuracy: 0.7211 - val_top_3_accuracy: 0.8721\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.10065 to 1.08998, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 51/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0951 - categorical_accuracy: 0.7178 - top_3_accuracy: 0.8714 - val_loss: 1.0742 - val_categorical_accuracy: 0.7262 - val_top_3_accuracy: 0.8738\n",
      "\n",
      "Epoch 00051: val_loss improved from 1.08998 to 1.07415, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 52/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0909 - categorical_accuracy: 0.7190 - top_3_accuracy: 0.8716 - val_loss: 1.0886 - val_categorical_accuracy: 0.7226 - val_top_3_accuracy: 0.8719\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.07415\n",
      "Epoch 53/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0845 - categorical_accuracy: 0.7206 - top_3_accuracy: 0.8728 - val_loss: 1.0820 - val_categorical_accuracy: 0.7241 - val_top_3_accuracy: 0.8740\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.07415\n",
      "Epoch 54/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0812 - categorical_accuracy: 0.7214 - top_3_accuracy: 0.8728 - val_loss: 1.0726 - val_categorical_accuracy: 0.7279 - val_top_3_accuracy: 0.8740\n",
      "\n",
      "Epoch 00054: val_loss improved from 1.07415 to 1.07262, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 55/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 1.0784 - categorical_accuracy: 0.7218 - top_3_accuracy: 0.8732 - val_loss: 1.0813 - val_categorical_accuracy: 0.7243 - val_top_3_accuracy: 0.8740\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.07262\n",
      "Epoch 56/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0738 - categorical_accuracy: 0.7227 - top_3_accuracy: 0.8740 - val_loss: 1.0612 - val_categorical_accuracy: 0.7293 - val_top_3_accuracy: 0.8766\n",
      "\n",
      "Epoch 00056: val_loss improved from 1.07262 to 1.06125, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 57/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0688 - categorical_accuracy: 0.7257 - top_3_accuracy: 0.8750 - val_loss: 1.0596 - val_categorical_accuracy: 0.7304 - val_top_3_accuracy: 0.8768\n",
      "\n",
      "Epoch 00057: val_loss improved from 1.06125 to 1.05957, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 58/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 1.0637 - categorical_accuracy: 0.7257 - top_3_accuracy: 0.8763 - val_loss: 1.0562 - val_categorical_accuracy: 0.7311 - val_top_3_accuracy: 0.8775\n",
      "\n",
      "Epoch 00058: val_loss improved from 1.05957 to 1.05623, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 59/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0634 - categorical_accuracy: 0.7260 - top_3_accuracy: 0.8757 - val_loss: 1.0776 - val_categorical_accuracy: 0.7256 - val_top_3_accuracy: 0.8744\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.05623\n",
      "Epoch 60/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 1.0593 - categorical_accuracy: 0.7273 - top_3_accuracy: 0.8765 - val_loss: 1.0614 - val_categorical_accuracy: 0.7290 - val_top_3_accuracy: 0.8761\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.05623\n",
      "Epoch 61/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0548 - categorical_accuracy: 0.7282 - top_3_accuracy: 0.8767 - val_loss: 1.0615 - val_categorical_accuracy: 0.7289 - val_top_3_accuracy: 0.8761\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.05623\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "Epoch 62/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 1.0382 - categorical_accuracy: 0.7320 - top_3_accuracy: 0.8797 - val_loss: 1.0473 - val_categorical_accuracy: 0.7345 - val_top_3_accuracy: 0.8782\n",
      "\n",
      "Epoch 00062: val_loss improved from 1.05623 to 1.04734, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 63/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 1.0362 - categorical_accuracy: 0.7321 - top_3_accuracy: 0.8797 - val_loss: 1.0546 - val_categorical_accuracy: 0.7307 - val_top_3_accuracy: 0.8774\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.04734\n",
      "Epoch 64/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0296 - categorical_accuracy: 0.7335 - top_3_accuracy: 0.8808 - val_loss: 1.0406 - val_categorical_accuracy: 0.7343 - val_top_3_accuracy: 0.8789\n",
      "\n",
      "Epoch 00064: val_loss improved from 1.04734 to 1.04056, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 65/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 1.0290 - categorical_accuracy: 0.7345 - top_3_accuracy: 0.8807 - val_loss: 1.0409 - val_categorical_accuracy: 0.7351 - val_top_3_accuracy: 0.8795\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.04056\n",
      "Epoch 66/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 1.0271 - categorical_accuracy: 0.7342 - top_3_accuracy: 0.8814 - val_loss: 1.0367 - val_categorical_accuracy: 0.7348 - val_top_3_accuracy: 0.8799\n",
      "\n",
      "Epoch 00066: val_loss improved from 1.04056 to 1.03671, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 67/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 1.0240 - categorical_accuracy: 0.7356 - top_3_accuracy: 0.8824 - val_loss: 1.0402 - val_categorical_accuracy: 0.7350 - val_top_3_accuracy: 0.8798\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.03671\n",
      "Epoch 68/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0259 - categorical_accuracy: 0.7348 - top_3_accuracy: 0.8813 - val_loss: 1.0468 - val_categorical_accuracy: 0.7343 - val_top_3_accuracy: 0.8780\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.03671\n",
      "Epoch 69/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0206 - categorical_accuracy: 0.7365 - top_3_accuracy: 0.8819 - val_loss: 1.0404 - val_categorical_accuracy: 0.7343 - val_top_3_accuracy: 0.8791\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.03671\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "Epoch 70/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0094 - categorical_accuracy: 0.7393 - top_3_accuracy: 0.8838 - val_loss: 1.0388 - val_categorical_accuracy: 0.7349 - val_top_3_accuracy: 0.8796\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.03671\n",
      "Epoch 71/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0082 - categorical_accuracy: 0.7391 - top_3_accuracy: 0.8834 - val_loss: 1.0286 - val_categorical_accuracy: 0.7381 - val_top_3_accuracy: 0.8813\n",
      "\n",
      "Epoch 00071: val_loss improved from 1.03671 to 1.02856, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 72/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0073 - categorical_accuracy: 0.7394 - top_3_accuracy: 0.8842 - val_loss: 1.0307 - val_categorical_accuracy: 0.7382 - val_top_3_accuracy: 0.8804\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.02856\n",
      "Epoch 73/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0056 - categorical_accuracy: 0.7396 - top_3_accuracy: 0.8844 - val_loss: 1.0333 - val_categorical_accuracy: 0.7363 - val_top_3_accuracy: 0.8807\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.02856\n",
      "Epoch 74/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0038 - categorical_accuracy: 0.7408 - top_3_accuracy: 0.8849 - val_loss: 1.0282 - val_categorical_accuracy: 0.7383 - val_top_3_accuracy: 0.8811\n",
      "\n",
      "Epoch 00074: val_loss improved from 1.02856 to 1.02822, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 75/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9996 - categorical_accuracy: 0.7414 - top_3_accuracy: 0.8851 - val_loss: 1.0281 - val_categorical_accuracy: 0.7379 - val_top_3_accuracy: 0.8817\n",
      "\n",
      "Epoch 00075: val_loss improved from 1.02822 to 1.02809, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 76/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9982 - categorical_accuracy: 0.7420 - top_3_accuracy: 0.8857 - val_loss: 1.0293 - val_categorical_accuracy: 0.7390 - val_top_3_accuracy: 0.8811\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.02809\n",
      "Epoch 77/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 1.0005 - categorical_accuracy: 0.7413 - top_3_accuracy: 0.8853 - val_loss: 1.0281 - val_categorical_accuracy: 0.7390 - val_top_3_accuracy: 0.8814\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.02809\n",
      "Epoch 78/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9975 - categorical_accuracy: 0.7420 - top_3_accuracy: 0.8858 - val_loss: 1.0257 - val_categorical_accuracy: 0.7391 - val_top_3_accuracy: 0.8816\n",
      "\n",
      "Epoch 00078: val_loss improved from 1.02809 to 1.02574, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 79/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9969 - categorical_accuracy: 0.7422 - top_3_accuracy: 0.8856 - val_loss: 1.0305 - val_categorical_accuracy: 0.7382 - val_top_3_accuracy: 0.8810\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.02574\n",
      "Epoch 80/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9959 - categorical_accuracy: 0.7419 - top_3_accuracy: 0.8856 - val_loss: 1.0262 - val_categorical_accuracy: 0.7391 - val_top_3_accuracy: 0.8818\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.02574\n",
      "Epoch 81/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9942 - categorical_accuracy: 0.7424 - top_3_accuracy: 0.8861 - val_loss: 1.0226 - val_categorical_accuracy: 0.7410 - val_top_3_accuracy: 0.8824\n",
      "\n",
      "Epoch 00081: val_loss improved from 1.02574 to 1.02257, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 82/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9897 - categorical_accuracy: 0.7438 - top_3_accuracy: 0.8862 - val_loss: 1.0280 - val_categorical_accuracy: 0.7383 - val_top_3_accuracy: 0.8816\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.02257\n",
      "Epoch 83/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9894 - categorical_accuracy: 0.7443 - top_3_accuracy: 0.8866 - val_loss: 1.0280 - val_categorical_accuracy: 0.7384 - val_top_3_accuracy: 0.8818\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.02257\n",
      "Epoch 84/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9902 - categorical_accuracy: 0.7436 - top_3_accuracy: 0.8865 - val_loss: 1.0308 - val_categorical_accuracy: 0.7383 - val_top_3_accuracy: 0.8810\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.02257\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "Epoch 85/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9839 - categorical_accuracy: 0.7456 - top_3_accuracy: 0.8876 - val_loss: 1.0233 - val_categorical_accuracy: 0.7400 - val_top_3_accuracy: 0.8823\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.02257\n",
      "Epoch 86/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9818 - categorical_accuracy: 0.7455 - top_3_accuracy: 0.8880 - val_loss: 1.0148 - val_categorical_accuracy: 0.7429 - val_top_3_accuracy: 0.8831\n",
      "\n",
      "Epoch 00086: val_loss improved from 1.02257 to 1.01477, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 87/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9802 - categorical_accuracy: 0.7459 - top_3_accuracy: 0.8884 - val_loss: 1.0139 - val_categorical_accuracy: 0.7412 - val_top_3_accuracy: 0.8830\n",
      "\n",
      "Epoch 00087: val_loss improved from 1.01477 to 1.01392, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 88/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9786 - categorical_accuracy: 0.7463 - top_3_accuracy: 0.8881 - val_loss: 1.0173 - val_categorical_accuracy: 0.7418 - val_top_3_accuracy: 0.8831\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.01392\n",
      "Epoch 89/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9755 - categorical_accuracy: 0.7476 - top_3_accuracy: 0.8880 - val_loss: 1.0203 - val_categorical_accuracy: 0.7420 - val_top_3_accuracy: 0.8826\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.01392\n",
      "Epoch 90/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9774 - categorical_accuracy: 0.7469 - top_3_accuracy: 0.8885 - val_loss: 1.0164 - val_categorical_accuracy: 0.7417 - val_top_3_accuracy: 0.8830\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.01392\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "Epoch 91/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 0.9730 - categorical_accuracy: 0.7474 - top_3_accuracy: 0.8893 - val_loss: 1.0190 - val_categorical_accuracy: 0.7412 - val_top_3_accuracy: 0.8835\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.01392\n",
      "Epoch 92/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9723 - categorical_accuracy: 0.7485 - top_3_accuracy: 0.8893 - val_loss: 1.0157 - val_categorical_accuracy: 0.7416 - val_top_3_accuracy: 0.8836\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.01392\n",
      "Epoch 93/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9726 - categorical_accuracy: 0.7491 - top_3_accuracy: 0.8891 - val_loss: 1.0141 - val_categorical_accuracy: 0.7430 - val_top_3_accuracy: 0.8840\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.01392\n",
      "Epoch 94/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9702 - categorical_accuracy: 0.7484 - top_3_accuracy: 0.8892 - val_loss: 1.0137 - val_categorical_accuracy: 0.7431 - val_top_3_accuracy: 0.8841\n",
      "\n",
      "Epoch 00094: val_loss improved from 1.01392 to 1.01371, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 95/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9703 - categorical_accuracy: 0.7483 - top_3_accuracy: 0.8890 - val_loss: 1.0161 - val_categorical_accuracy: 0.7424 - val_top_3_accuracy: 0.8833\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.01371\n",
      "Epoch 96/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9683 - categorical_accuracy: 0.7488 - top_3_accuracy: 0.8899 - val_loss: 1.0169 - val_categorical_accuracy: 0.7424 - val_top_3_accuracy: 0.8833\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.01371\n",
      "Epoch 97/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9662 - categorical_accuracy: 0.7494 - top_3_accuracy: 0.8897 - val_loss: 1.0148 - val_categorical_accuracy: 0.7425 - val_top_3_accuracy: 0.8838\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.01371\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "Epoch 98/150\n",
      "262500/262500 [==============================] - 108s 412us/step - loss: 0.9670 - categorical_accuracy: 0.7494 - top_3_accuracy: 0.8889 - val_loss: 1.0155 - val_categorical_accuracy: 0.7427 - val_top_3_accuracy: 0.8836\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.01371\n",
      "Epoch 99/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9677 - categorical_accuracy: 0.7488 - top_3_accuracy: 0.8900 - val_loss: 1.0175 - val_categorical_accuracy: 0.7414 - val_top_3_accuracy: 0.8834\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.01371\n",
      "Epoch 100/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9672 - categorical_accuracy: 0.7489 - top_3_accuracy: 0.8895 - val_loss: 1.0122 - val_categorical_accuracy: 0.7427 - val_top_3_accuracy: 0.8832\n",
      "\n",
      "Epoch 00100: val_loss improved from 1.01371 to 1.01217, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 101/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9653 - categorical_accuracy: 0.7495 - top_3_accuracy: 0.8899 - val_loss: 1.0147 - val_categorical_accuracy: 0.7433 - val_top_3_accuracy: 0.8832\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.01217\n",
      "Epoch 102/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9666 - categorical_accuracy: 0.7497 - top_3_accuracy: 0.8897 - val_loss: 1.0152 - val_categorical_accuracy: 0.7426 - val_top_3_accuracy: 0.8832\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.01217\n",
      "Epoch 103/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9647 - categorical_accuracy: 0.7495 - top_3_accuracy: 0.8901 - val_loss: 1.0129 - val_categorical_accuracy: 0.7430 - val_top_3_accuracy: 0.8830\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.01217\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "Epoch 104/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9650 - categorical_accuracy: 0.7498 - top_3_accuracy: 0.8894 - val_loss: 1.0117 - val_categorical_accuracy: 0.7436 - val_top_3_accuracy: 0.8841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00104: val_loss improved from 1.01217 to 1.01166, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 105/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9636 - categorical_accuracy: 0.7500 - top_3_accuracy: 0.8903 - val_loss: 1.0135 - val_categorical_accuracy: 0.7432 - val_top_3_accuracy: 0.8837\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.01166\n",
      "Epoch 106/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9613 - categorical_accuracy: 0.7511 - top_3_accuracy: 0.8906 - val_loss: 1.0129 - val_categorical_accuracy: 0.7430 - val_top_3_accuracy: 0.8842\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.01166\n",
      "Epoch 107/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9625 - categorical_accuracy: 0.7509 - top_3_accuracy: 0.8907 - val_loss: 1.0120 - val_categorical_accuracy: 0.7433 - val_top_3_accuracy: 0.8839\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.01166\n",
      "Epoch 108/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9618 - categorical_accuracy: 0.7504 - top_3_accuracy: 0.8903 - val_loss: 1.0130 - val_categorical_accuracy: 0.7429 - val_top_3_accuracy: 0.8836\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.01166\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "Epoch 109/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9651 - categorical_accuracy: 0.7497 - top_3_accuracy: 0.8897 - val_loss: 1.0117 - val_categorical_accuracy: 0.7433 - val_top_3_accuracy: 0.8837\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.01166\n",
      "Epoch 110/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9618 - categorical_accuracy: 0.7509 - top_3_accuracy: 0.8899 - val_loss: 1.0135 - val_categorical_accuracy: 0.7428 - val_top_3_accuracy: 0.8836\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.01166\n",
      "Epoch 111/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9607 - categorical_accuracy: 0.7512 - top_3_accuracy: 0.8904 - val_loss: 1.0131 - val_categorical_accuracy: 0.7427 - val_top_3_accuracy: 0.8839\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.01166\n",
      "Epoch 112/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9608 - categorical_accuracy: 0.7516 - top_3_accuracy: 0.8909 - val_loss: 1.0126 - val_categorical_accuracy: 0.7428 - val_top_3_accuracy: 0.8840\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.01166\n",
      "Epoch 113/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9604 - categorical_accuracy: 0.7504 - top_3_accuracy: 0.8898 - val_loss: 1.0116 - val_categorical_accuracy: 0.7436 - val_top_3_accuracy: 0.8837\n",
      "\n",
      "Epoch 00113: val_loss improved from 1.01166 to 1.01164, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "Epoch 114/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9621 - categorical_accuracy: 0.7507 - top_3_accuracy: 0.8907 - val_loss: 1.0122 - val_categorical_accuracy: 0.7434 - val_top_3_accuracy: 0.8839\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.01164\n",
      "Epoch 115/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9595 - categorical_accuracy: 0.7508 - top_3_accuracy: 0.8908 - val_loss: 1.0131 - val_categorical_accuracy: 0.7430 - val_top_3_accuracy: 0.8837\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.01164\n",
      "Epoch 116/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9607 - categorical_accuracy: 0.7514 - top_3_accuracy: 0.8904 - val_loss: 1.0124 - val_categorical_accuracy: 0.7430 - val_top_3_accuracy: 0.8839\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.01164\n",
      "Epoch 117/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9589 - categorical_accuracy: 0.7515 - top_3_accuracy: 0.8908 - val_loss: 1.0114 - val_categorical_accuracy: 0.7436 - val_top_3_accuracy: 0.8842\n",
      "\n",
      "Epoch 00117: val_loss improved from 1.01164 to 1.01142, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 118/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9604 - categorical_accuracy: 0.7509 - top_3_accuracy: 0.8902 - val_loss: 1.0103 - val_categorical_accuracy: 0.7436 - val_top_3_accuracy: 0.8843\n",
      "\n",
      "Epoch 00118: val_loss improved from 1.01142 to 1.01028, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 119/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9598 - categorical_accuracy: 0.7506 - top_3_accuracy: 0.8908 - val_loss: 1.0123 - val_categorical_accuracy: 0.7433 - val_top_3_accuracy: 0.8836\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.01028\n",
      "Epoch 120/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9601 - categorical_accuracy: 0.7511 - top_3_accuracy: 0.8904 - val_loss: 1.0116 - val_categorical_accuracy: 0.7435 - val_top_3_accuracy: 0.8836\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.01028\n",
      "Epoch 121/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9598 - categorical_accuracy: 0.7514 - top_3_accuracy: 0.8913 - val_loss: 1.0104 - val_categorical_accuracy: 0.7435 - val_top_3_accuracy: 0.8841\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.01028\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 5e-06.\n",
      "Epoch 122/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9613 - categorical_accuracy: 0.7507 - top_3_accuracy: 0.8912 - val_loss: 1.0107 - val_categorical_accuracy: 0.7437 - val_top_3_accuracy: 0.8842\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.01028\n",
      "Epoch 123/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9594 - categorical_accuracy: 0.7503 - top_3_accuracy: 0.8914 - val_loss: 1.0105 - val_categorical_accuracy: 0.7436 - val_top_3_accuracy: 0.8840\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.01028\n",
      "Epoch 124/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9604 - categorical_accuracy: 0.7520 - top_3_accuracy: 0.8909 - val_loss: 1.0108 - val_categorical_accuracy: 0.7438 - val_top_3_accuracy: 0.8840\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.01028\n",
      "Epoch 125/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9609 - categorical_accuracy: 0.7507 - top_3_accuracy: 0.8900 - val_loss: 1.0110 - val_categorical_accuracy: 0.7435 - val_top_3_accuracy: 0.8841\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.01028\n",
      "Epoch 126/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9602 - categorical_accuracy: 0.7514 - top_3_accuracy: 0.8904 - val_loss: 1.0117 - val_categorical_accuracy: 0.7435 - val_top_3_accuracy: 0.8839\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.01028\n",
      "Epoch 127/150\n",
      "262500/262500 [==============================] - 108s 410us/step - loss: 0.9593 - categorical_accuracy: 0.7517 - top_3_accuracy: 0.8904 - val_loss: 1.0120 - val_categorical_accuracy: 0.7434 - val_top_3_accuracy: 0.8838\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.01028\n",
      "Epoch 128/150\n",
      "262500/262500 [==============================] - 108s 411us/step - loss: 0.9581 - categorical_accuracy: 0.7515 - top_3_accuracy: 0.8915 - val_loss: 1.0121 - val_categorical_accuracy: 0.7434 - val_top_3_accuracy: 0.8839\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.01028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78e8b25908>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "stroke_read_model.fit(train_X, train_y,\n",
    "                      validation_data = (valid_X, valid_y), \n",
    "                      batch_size = batch_size,\n",
    "                      epochs = 150,\n",
    "                      callbacks = callbacks_list)\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "a7eb5b62-cf57-4380-8786-9ddc05be658f",
    "_uuid": "858059b6c16d81f86460bef8fcf595e0d68d12b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 5s 117us/step\n",
      "Accuracy: 74.3%, Top 3 Accuracy 88.6%\n"
     ]
    }
   ],
   "source": [
    "stroke_read_model.load_weights(weight_path)\n",
    "lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
