{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "522ce330-d4f9-4fbe-9684-4b65fd684cca",
    "_uuid": "174484daa5f084ce4970f5048d3e05d2c4429787"
   },
   "source": [
    "# Overview\n",
    "The notebook is modified from one that was made for the [Quick, Draw Dataset](https://www.kaggle.com/google/tinyquickdraw), it would actually be interesting to see how beneficial a transfer learning approach using that data as a starting point could be.\n",
    "\n",
    "## This Notebook\n",
    "The notebook takes and preprocesses the data from the QuickDraw Competition step (strokes) and trains an LSTM. The outcome variable (y) is always the same (category). The stroke-based LSTM. The model takes the stroke data and 'preprocesses' it a bit using 1D convolutions and then uses two stacked LSTMs followed by two dense layers to make the classification. The model can be thought to 'read' the drawing stroke by stroke.\n",
    "\n",
    "## Fun Models\n",
    "\n",
    "After the classification models, we try to build a few models to understand what the LSTM actually does. Here we experiment step by step to see how the prediction changes with each stop\n",
    "\n",
    "### Next Steps\n",
    "The next steps could be\n",
    "- use more data to train\n",
    "- include the country code (different countries draw different things, different ways)\n",
    "- more complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8ccded02a5f2c4a9d9ee2f7688114bcd2e1f11a"
   },
   "source": [
    "### Model Parameters\n",
    "Here we keep track of the relevant parameters for the data preprocessing, model construction and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "8b08fbab2000a563b388f126eac74362641e497c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "STROKE_COUNT = 196\n",
    "TRAIN_SAMPLES = 3000\n",
    "VALID_SAMPLES = 600\n",
    "TEST_SAMPLES = 500\n",
    "NUM_CLASSES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(69)\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from glob import glob\n",
    "import gc\n",
    "gc.enable()\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "base_dir = os.path.join('..', 'input')\n",
    "test_path = os.path.join(base_dir, 'test_simplified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "7acacf8e960084782425ef1a1a3fd532a240ad48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "ALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\n",
    "COL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
    "\n",
    "def _stack_it(raw_strokes):\n",
    "    \"\"\"preprocess the string and make \n",
    "    a standard Nx3 stroke vector\"\"\"\n",
    "    stroke_vec = literal_eval(raw_strokes) # string->list\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,i)  \n",
    "     for i,(x,y) in enumerate(stroke_vec) \n",
    "     for xi,yi in zip(x,y)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "    # pad the strokes with zeros\n",
    "    x = pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "                         maxlen=STROKE_COUNT, \n",
    "                         padding='post').swapaxes(0, 1)\n",
    "    return x\n",
    "\n",
    "def read_batch(samples=5, \n",
    "               start_row=0,\n",
    "               max_rows = 1000):\n",
    "    \"\"\"\n",
    "    load and process the csv files\n",
    "    this function is horribly inefficient but simple\n",
    "    \"\"\"\n",
    "    out_df_list = []\n",
    "    for c_path in ALL_TRAIN_PATHS[:NUM_CLASSES]:\n",
    "        c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n",
    "        c_df.columns=COL_NAMES\n",
    "        out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n",
    "    full_df = pd.concat(out_df_list)\n",
    "    full_df['drawing'] = full_df['drawing'].\\\n",
    "        map(_stack_it)\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ec1854b21b36cd2fd7f7d0717aaa8da32506a6a"
   },
   "source": [
    "# Reading and Parsing\n",
    "Since it is too much data (23GB) to read in at once, we just take a portion of it for training, validation and hold-out testing. This should give us an idea about how well the model works, but leaves lots of room for improvement later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words 50 => The Great Wall of China, angel, banana, bathtub, bear, bridge, bush, butterfly, cake, calendar, cat, ceiling fan, chandelier, church, dog, dolphin, door, dresser, drill, elbow, elephant, garden, grapes, hockey puck, hourglass, house, house plant, key, marker, megaphone, mermaid, monkey, mosquito, ocean, paper clip, sink, sleeping bag, snail, spreadsheet, t-shirt, television, tent, toothpaste, traffic light, triangle, trumpet, van, washing machine, wheel, wine glass\n"
     ]
    }
   ],
   "source": [
    "train_args = dict(samples=TRAIN_SAMPLES, \n",
    "                  start_row=0, \n",
    "                  max_rows=int(TRAIN_SAMPLES*1.5))\n",
    "valid_args = dict(samples=VALID_SAMPLES, \n",
    "                  start_row=train_args['max_rows']+1, \n",
    "                  max_rows=VALID_SAMPLES+25)\n",
    "test_args = dict(samples=TEST_SAMPLES, \n",
    "                 start_row=valid_args['max_rows']+train_args['max_rows']+1, \n",
    "                 max_rows=TEST_SAMPLES+25)\n",
    "train_df = read_batch(**train_args)\n",
    "valid_df = read_batch(**valid_args)\n",
    "test_df = read_batch(**test_args)\n",
    "word_encoder = LabelEncoder()\n",
    "word_encoder.fit(train_df['word'])\n",
    "print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d29237e-ece3-4dfd-9095-475296f4a608",
    "_uuid": "8bae16a4973a215861fbb536a602c4f5abf3b4bf"
   },
   "source": [
    "# Stroke-based Classification\n",
    "Here we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "ff5ddced-d77e-473f-899d-82cf11ad2bd9",
    "_uuid": "409468f1d5abd17b819482473a4f354a61f8d7ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 196, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_Xy(in_df):\n",
    "    X = np.stack(in_df['drawing'], 0)\n",
    "    y = to_categorical(word_encoder.transform(in_df['word'].values))\n",
    "    return X, y\n",
    "train_X, train_y = get_Xy(train_df)\n",
    "valid_X, valid_y = get_Xy(valid_df)\n",
    "test_X, test_y = get_Xy(test_df)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "56240ed9-42b0-4f62-b3d1-f92017f04e30",
    "_uuid": "5cc79204a0a1da048d1d58ba8dfdafd0af3ebcb8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
    "# rand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\n",
    "# for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
    "#     test_arr = train_X[c_id]\n",
    "#     test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "#     lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "#     for i in np.unique(lab_idx):\n",
    "#         c_ax.plot(test_arr[lab_idx==i,0], \n",
    "#                 np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
    "#     c_ax.axis('off')\n",
    "#     c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e1d5bba-0fb4-432c-bd0b-ad69be0ef9ac",
    "_uuid": "b4a087a17798c2ec8eb520bc916bcad38d4ebff2",
    "collapsed": true
   },
   "source": [
    "# LSTM to Parse Strokes\n",
    "The model suggeted from the tutorial is\n",
    "\n",
    "![Suggested Model](https://www.tensorflow.org/versions/master/images/quickdraw_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "d65490e7-302e-4232-afe7-4e9499010e31",
    "_uuid": "ba9d55554ba9e4177df5f0645ca1e0f5e4393ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_4 (Batch (None, None, 3)           12        \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, None, 48)          768       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, None, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, None, 64)          15424     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 96)          18528     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, None, 96)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_7 (CuDNNLSTM)     (None, None, 128)         115712    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_8 (CuDNNLSTM)     (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                25650     \n",
      "=================================================================\n",
      "Total params: 374,238\n",
      "Trainable params: 374,232\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout\n",
    "if len(get_available_gpus())>0:\n",
    "    # https://twitter.com/fchollet/status/918170264608817152?lang=en\n",
    "    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances\n",
    "stroke_read_model = Sequential()\n",
    "stroke_read_model.add(BatchNormalization(input_shape = (None,)+train_X.shape[2:]))\n",
    "# filter count and length are taken from the script https://github.com/tensorflow/models/blob/master/tutorials/rnn/quickdraw/train_model.py\n",
    "stroke_read_model.add(Conv1D(48, (5,)))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Conv1D(64, (5,)))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Conv1D(96, (3,)))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = True))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = False))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Dense(512))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Dense(len(word_encoder.classes_), activation = 'softmax'))\n",
    "stroke_read_model.compile(optimizer = 'adam', \n",
    "                          loss = 'categorical_crossentropy', \n",
    "                          metrics = ['categorical_accuracy', top_3_accuracy])\n",
    "stroke_read_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "2a549512-a9d9-4afd-b748-3e1c3296e193",
    "_uuid": "5fda10b30c47a8cf6ea822ed0a4a1d7cd2c81195"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/callbacks.py:1062: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=3, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=3, min_lr=0.000005)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=10) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "1d2398797580ff8619651dc701e18d02f986f57e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import Callback\n",
    "# class OutputClearNEpoch(Callback):\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         current = logs.get(self.monitor)\n",
    "#         if epoch % 5 == 0:\n",
    "#             clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "825b3af8-9451-487b-a1e1-538f2f1489e1",
    "_uuid": "ed2fc26af74aed1a93bbc253d61b72db5a81f5cc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150000 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "150000/150000 [==============================] - 61s 404us/step - loss: 3.9116 - categorical_accuracy: 0.0205 - top_3_accuracy: 0.0612 - val_loss: 3.9103 - val_categorical_accuracy: 0.0211 - val_top_3_accuracy: 0.0627\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.91030, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 2/100\n",
      "150000/150000 [==============================] - 59s 390us/step - loss: 3.9222 - categorical_accuracy: 0.0212 - top_3_accuracy: 0.0625 - val_loss: 3.9978 - val_categorical_accuracy: 0.0201 - val_top_3_accuracy: 0.0602\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 3.91030\n",
      "Epoch 3/100\n",
      "150000/150000 [==============================] - 59s 390us/step - loss: 3.9249 - categorical_accuracy: 0.0203 - top_3_accuracy: 0.0602 - val_loss: 3.9096 - val_categorical_accuracy: 0.0207 - val_top_3_accuracy: 0.0618\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.91030 to 3.90957, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 4/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 3.9100 - categorical_accuracy: 0.0204 - top_3_accuracy: 0.0603 - val_loss: 3.9090 - val_categorical_accuracy: 0.0208 - val_top_3_accuracy: 0.0618\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.90957 to 3.90901, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 5/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 3.8333 - categorical_accuracy: 0.0283 - top_3_accuracy: 0.0835 - val_loss: 3.6716 - val_categorical_accuracy: 0.0399 - val_top_3_accuracy: 0.1230\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.90901 to 3.67157, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 6/100\n",
      "150000/150000 [==============================] - 59s 390us/step - loss: 3.6564 - categorical_accuracy: 0.0451 - top_3_accuracy: 0.1248 - val_loss: 3.5346 - val_categorical_accuracy: 0.0612 - val_top_3_accuracy: 0.1633\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.67157 to 3.53462, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 7/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 3.4889 - categorical_accuracy: 0.0712 - top_3_accuracy: 0.1891 - val_loss: 3.3129 - val_categorical_accuracy: 0.1214 - val_top_3_accuracy: 0.2745\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.53462 to 3.31288, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 8/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 3.2026 - categorical_accuracy: 0.1324 - top_3_accuracy: 0.3027 - val_loss: 2.9799 - val_categorical_accuracy: 0.1943 - val_top_3_accuracy: 0.3927\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.31288 to 2.97987, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 9/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 2.8338 - categorical_accuracy: 0.2213 - top_3_accuracy: 0.4394 - val_loss: 2.4650 - val_categorical_accuracy: 0.3109 - val_top_3_accuracy: 0.5565\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.97987 to 2.46499, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 10/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 2.4193 - categorical_accuracy: 0.3292 - top_3_accuracy: 0.5743 - val_loss: 2.0625 - val_categorical_accuracy: 0.4181 - val_top_3_accuracy: 0.6720\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.46499 to 2.06251, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 11/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 2.1298 - categorical_accuracy: 0.4088 - top_3_accuracy: 0.6535 - val_loss: 1.8328 - val_categorical_accuracy: 0.4871 - val_top_3_accuracy: 0.7305\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.06251 to 1.83276, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 12/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 1.8995 - categorical_accuracy: 0.4735 - top_3_accuracy: 0.7106 - val_loss: 1.6037 - val_categorical_accuracy: 0.5530 - val_top_3_accuracy: 0.7741\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.83276 to 1.60367, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 13/100\n",
      "150000/150000 [==============================] - 59s 390us/step - loss: 1.7373 - categorical_accuracy: 0.5180 - top_3_accuracy: 0.7449 - val_loss: 1.5050 - val_categorical_accuracy: 0.5851 - val_top_3_accuracy: 0.7995\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.60367 to 1.50499, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 14/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 1.6009 - categorical_accuracy: 0.5576 - top_3_accuracy: 0.7753 - val_loss: 1.3678 - val_categorical_accuracy: 0.6260 - val_top_3_accuracy: 0.8210\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.50499 to 1.36784, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 15/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 1.5061 - categorical_accuracy: 0.5838 - top_3_accuracy: 0.7931 - val_loss: 1.2974 - val_categorical_accuracy: 0.6483 - val_top_3_accuracy: 0.8339\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.36784 to 1.29740, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 16/100\n",
      "150000/150000 [==============================] - 59s 395us/step - loss: 1.4120 - categorical_accuracy: 0.6115 - top_3_accuracy: 0.8106 - val_loss: 1.2146 - val_categorical_accuracy: 0.6680 - val_top_3_accuracy: 0.8489\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.29740 to 1.21459, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 17/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 1.3552 - categorical_accuracy: 0.6266 - top_3_accuracy: 0.8209 - val_loss: 1.1476 - val_categorical_accuracy: 0.6864 - val_top_3_accuracy: 0.8601\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.21459 to 1.14765, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 18/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 1.2929 - categorical_accuracy: 0.6439 - top_3_accuracy: 0.8318 - val_loss: 1.1100 - val_categorical_accuracy: 0.6969 - val_top_3_accuracy: 0.8674\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.14765 to 1.10997, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 19/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 1.2435 - categorical_accuracy: 0.6586 - top_3_accuracy: 0.8410 - val_loss: 1.0860 - val_categorical_accuracy: 0.7048 - val_top_3_accuracy: 0.8705\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.10997 to 1.08595, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 20/100\n",
      "150000/150000 [==============================] - 58s 390us/step - loss: 1.1993 - categorical_accuracy: 0.6710 - top_3_accuracy: 0.8482 - val_loss: 1.0554 - val_categorical_accuracy: 0.7111 - val_top_3_accuracy: 0.8749\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.08595 to 1.05542, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 21/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 1.1658 - categorical_accuracy: 0.6799 - top_3_accuracy: 0.8542 - val_loss: 1.0490 - val_categorical_accuracy: 0.7157 - val_top_3_accuracy: 0.8772\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.05542 to 1.04902, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 22/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 1.1324 - categorical_accuracy: 0.6892 - top_3_accuracy: 0.8588 - val_loss: 1.0307 - val_categorical_accuracy: 0.7185 - val_top_3_accuracy: 0.8776\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.04902 to 1.03068, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 23/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 1.1024 - categorical_accuracy: 0.6964 - top_3_accuracy: 0.8645 - val_loss: 1.0011 - val_categorical_accuracy: 0.7272 - val_top_3_accuracy: 0.8814\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.03068 to 1.00109, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 24/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 1.0807 - categorical_accuracy: 0.7023 - top_3_accuracy: 0.8676 - val_loss: 0.9585 - val_categorical_accuracy: 0.7389 - val_top_3_accuracy: 0.8881\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.00109 to 0.95853, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 25/100\n",
      "150000/150000 [==============================] - 58s 390us/step - loss: 1.0513 - categorical_accuracy: 0.7109 - top_3_accuracy: 0.8728 - val_loss: 0.9386 - val_categorical_accuracy: 0.7419 - val_top_3_accuracy: 0.8906\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.95853 to 0.93861, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 26/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 1.0295 - categorical_accuracy: 0.7166 - top_3_accuracy: 0.8760 - val_loss: 0.9375 - val_categorical_accuracy: 0.7412 - val_top_3_accuracy: 0.8916\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.93861 to 0.93754, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 27/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 1.0131 - categorical_accuracy: 0.7207 - top_3_accuracy: 0.8783 - val_loss: 0.9028 - val_categorical_accuracy: 0.7518 - val_top_3_accuracy: 0.8980\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.93754 to 0.90277, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 28/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.9914 - categorical_accuracy: 0.7260 - top_3_accuracy: 0.8818 - val_loss: 0.9462 - val_categorical_accuracy: 0.7468 - val_top_3_accuracy: 0.8936\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.90277\n",
      "Epoch 29/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.9661 - categorical_accuracy: 0.7325 - top_3_accuracy: 0.8854 - val_loss: 0.8903 - val_categorical_accuracy: 0.7561 - val_top_3_accuracy: 0.8988\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.90277 to 0.89028, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 30/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.9526 - categorical_accuracy: 0.7369 - top_3_accuracy: 0.8870 - val_loss: 0.8786 - val_categorical_accuracy: 0.7637 - val_top_3_accuracy: 0.9013\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.89028 to 0.87860, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 31/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.9420 - categorical_accuracy: 0.7394 - top_3_accuracy: 0.8888 - val_loss: 0.8716 - val_categorical_accuracy: 0.7647 - val_top_3_accuracy: 0.9012\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.87860 to 0.87159, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 32/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.9279 - categorical_accuracy: 0.7435 - top_3_accuracy: 0.8920 - val_loss: 0.8555 - val_categorical_accuracy: 0.7688 - val_top_3_accuracy: 0.9038\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.87159 to 0.85552, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 33/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.9119 - categorical_accuracy: 0.7479 - top_3_accuracy: 0.8937 - val_loss: 0.8661 - val_categorical_accuracy: 0.7674 - val_top_3_accuracy: 0.9047\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.85552\n",
      "Epoch 34/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.8952 - categorical_accuracy: 0.7523 - top_3_accuracy: 0.8964 - val_loss: 0.8384 - val_categorical_accuracy: 0.7742 - val_top_3_accuracy: 0.9064\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.85552 to 0.83840, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 35/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.8851 - categorical_accuracy: 0.7543 - top_3_accuracy: 0.8974 - val_loss: 0.8319 - val_categorical_accuracy: 0.7764 - val_top_3_accuracy: 0.9075\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.83840 to 0.83190, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 36/100\n",
      "150000/150000 [==============================] - 59s 394us/step - loss: 0.8671 - categorical_accuracy: 0.7604 - top_3_accuracy: 0.8998 - val_loss: 0.8100 - val_categorical_accuracy: 0.7781 - val_top_3_accuracy: 0.9092\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.83190 to 0.80996, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 37/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.8642 - categorical_accuracy: 0.7598 - top_3_accuracy: 0.9004 - val_loss: 0.8248 - val_categorical_accuracy: 0.7765 - val_top_3_accuracy: 0.9086\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.80996\n",
      "Epoch 38/100\n",
      "150000/150000 [==============================] - 59s 390us/step - loss: 0.8524 - categorical_accuracy: 0.7637 - top_3_accuracy: 0.9025 - val_loss: 0.8040 - val_categorical_accuracy: 0.7812 - val_top_3_accuracy: 0.9112\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.80996 to 0.80399, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 39/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.8412 - categorical_accuracy: 0.7665 - top_3_accuracy: 0.9042 - val_loss: 0.8124 - val_categorical_accuracy: 0.7785 - val_top_3_accuracy: 0.9103\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.80399\n",
      "Epoch 40/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.8317 - categorical_accuracy: 0.7693 - top_3_accuracy: 0.9058 - val_loss: 0.7910 - val_categorical_accuracy: 0.7841 - val_top_3_accuracy: 0.9117\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.80399 to 0.79097, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 41/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.8224 - categorical_accuracy: 0.7721 - top_3_accuracy: 0.9064 - val_loss: 0.8177 - val_categorical_accuracy: 0.7811 - val_top_3_accuracy: 0.9105\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.79097\n",
      "Epoch 42/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.8105 - categorical_accuracy: 0.7754 - top_3_accuracy: 0.9091 - val_loss: 0.8048 - val_categorical_accuracy: 0.7844 - val_top_3_accuracy: 0.9117\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.79097\n",
      "Epoch 43/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.8067 - categorical_accuracy: 0.7764 - top_3_accuracy: 0.9090 - val_loss: 0.7887 - val_categorical_accuracy: 0.7883 - val_top_3_accuracy: 0.9142\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.79097 to 0.78874, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 44/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.8000 - categorical_accuracy: 0.7779 - top_3_accuracy: 0.9098 - val_loss: 0.8023 - val_categorical_accuracy: 0.7826 - val_top_3_accuracy: 0.9110\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.78874\n",
      "Epoch 45/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.7950 - categorical_accuracy: 0.7786 - top_3_accuracy: 0.9110 - val_loss: 0.7876 - val_categorical_accuracy: 0.7878 - val_top_3_accuracy: 0.9126\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.78874 to 0.78756, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 46/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.7822 - categorical_accuracy: 0.7821 - top_3_accuracy: 0.9125 - val_loss: 0.7727 - val_categorical_accuracy: 0.7914 - val_top_3_accuracy: 0.9135\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.78756 to 0.77270, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 47/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.7772 - categorical_accuracy: 0.7849 - top_3_accuracy: 0.9129 - val_loss: 0.7772 - val_categorical_accuracy: 0.7903 - val_top_3_accuracy: 0.9140\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.77270\n",
      "Epoch 48/100\n",
      "150000/150000 [==============================] - 59s 394us/step - loss: 0.7669 - categorical_accuracy: 0.7866 - top_3_accuracy: 0.9146 - val_loss: 0.7717 - val_categorical_accuracy: 0.7916 - val_top_3_accuracy: 0.9153\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.77270 to 0.77169, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 49/100\n",
      "150000/150000 [==============================] - 59s 394us/step - loss: 0.7643 - categorical_accuracy: 0.7874 - top_3_accuracy: 0.9142 - val_loss: 0.7685 - val_categorical_accuracy: 0.7946 - val_top_3_accuracy: 0.9161\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.77169 to 0.76853, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 50/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.7592 - categorical_accuracy: 0.7895 - top_3_accuracy: 0.9153 - val_loss: 0.7524 - val_categorical_accuracy: 0.7963 - val_top_3_accuracy: 0.9195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: val_loss improved from 0.76853 to 0.75238, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 51/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.7475 - categorical_accuracy: 0.7933 - top_3_accuracy: 0.9174 - val_loss: 0.7659 - val_categorical_accuracy: 0.7940 - val_top_3_accuracy: 0.9149\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.75238\n",
      "Epoch 52/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.7425 - categorical_accuracy: 0.7929 - top_3_accuracy: 0.9183 - val_loss: 0.7649 - val_categorical_accuracy: 0.7945 - val_top_3_accuracy: 0.9157\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.75238\n",
      "Epoch 53/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.7384 - categorical_accuracy: 0.7947 - top_3_accuracy: 0.9187 - val_loss: 0.7565 - val_categorical_accuracy: 0.7993 - val_top_3_accuracy: 0.9180\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.75238\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Epoch 54/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.7102 - categorical_accuracy: 0.8028 - top_3_accuracy: 0.9219 - val_loss: 0.7300 - val_categorical_accuracy: 0.8053 - val_top_3_accuracy: 0.9223\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.75238 to 0.72998, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 55/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.7009 - categorical_accuracy: 0.8048 - top_3_accuracy: 0.9238 - val_loss: 0.7388 - val_categorical_accuracy: 0.8030 - val_top_3_accuracy: 0.9199\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.72998\n",
      "Epoch 56/100\n",
      "150000/150000 [==============================] - 58s 389us/step - loss: 0.6983 - categorical_accuracy: 0.8054 - top_3_accuracy: 0.9235 - val_loss: 0.7374 - val_categorical_accuracy: 0.8045 - val_top_3_accuracy: 0.9188\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.72998\n",
      "Epoch 57/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6913 - categorical_accuracy: 0.8074 - top_3_accuracy: 0.9245 - val_loss: 0.7425 - val_categorical_accuracy: 0.8043 - val_top_3_accuracy: 0.9208\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.72998\n",
      "Epoch 58/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.6906 - categorical_accuracy: 0.8070 - top_3_accuracy: 0.9246 - val_loss: 0.7351 - val_categorical_accuracy: 0.8069 - val_top_3_accuracy: 0.9207\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.72998\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "Epoch 59/100\n",
      "150000/150000 [==============================] - 59s 394us/step - loss: 0.6716 - categorical_accuracy: 0.8131 - top_3_accuracy: 0.9271 - val_loss: 0.7278 - val_categorical_accuracy: 0.8079 - val_top_3_accuracy: 0.9228\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.72998 to 0.72776, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 60/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6691 - categorical_accuracy: 0.8141 - top_3_accuracy: 0.9271 - val_loss: 0.7191 - val_categorical_accuracy: 0.8115 - val_top_3_accuracy: 0.9226\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.72776 to 0.71908, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 61/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.6659 - categorical_accuracy: 0.8132 - top_3_accuracy: 0.9285 - val_loss: 0.7271 - val_categorical_accuracy: 0.8084 - val_top_3_accuracy: 0.9229\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.71908\n",
      "Epoch 62/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6605 - categorical_accuracy: 0.8159 - top_3_accuracy: 0.9287 - val_loss: 0.7247 - val_categorical_accuracy: 0.8087 - val_top_3_accuracy: 0.9230\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.71908\n",
      "Epoch 63/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.6608 - categorical_accuracy: 0.8154 - top_3_accuracy: 0.9290 - val_loss: 0.7315 - val_categorical_accuracy: 0.8078 - val_top_3_accuracy: 0.9223\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.71908\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "Epoch 64/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6501 - categorical_accuracy: 0.8188 - top_3_accuracy: 0.9297 - val_loss: 0.7221 - val_categorical_accuracy: 0.8097 - val_top_3_accuracy: 0.9235\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.71908\n",
      "Epoch 65/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.6464 - categorical_accuracy: 0.8188 - top_3_accuracy: 0.9305 - val_loss: 0.7174 - val_categorical_accuracy: 0.8120 - val_top_3_accuracy: 0.9240\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.71908 to 0.71745, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 66/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6469 - categorical_accuracy: 0.8189 - top_3_accuracy: 0.9298 - val_loss: 0.7199 - val_categorical_accuracy: 0.8109 - val_top_3_accuracy: 0.9244\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.71745\n",
      "Epoch 67/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6428 - categorical_accuracy: 0.8201 - top_3_accuracy: 0.9308 - val_loss: 0.7251 - val_categorical_accuracy: 0.8102 - val_top_3_accuracy: 0.9236\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.71745\n",
      "Epoch 68/100\n",
      "150000/150000 [==============================] - 59s 394us/step - loss: 0.6411 - categorical_accuracy: 0.8210 - top_3_accuracy: 0.9313 - val_loss: 0.7156 - val_categorical_accuracy: 0.8104 - val_top_3_accuracy: 0.9241\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.71745 to 0.71557, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 69/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.6406 - categorical_accuracy: 0.8213 - top_3_accuracy: 0.9312 - val_loss: 0.7237 - val_categorical_accuracy: 0.8115 - val_top_3_accuracy: 0.9239\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.71557\n",
      "Epoch 70/100\n",
      "150000/150000 [==============================] - 58s 389us/step - loss: 0.6372 - categorical_accuracy: 0.8210 - top_3_accuracy: 0.9315 - val_loss: 0.7235 - val_categorical_accuracy: 0.8098 - val_top_3_accuracy: 0.9230\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.71557\n",
      "Epoch 71/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6381 - categorical_accuracy: 0.8212 - top_3_accuracy: 0.9315 - val_loss: 0.7170 - val_categorical_accuracy: 0.8115 - val_top_3_accuracy: 0.9231\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.71557\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "Epoch 72/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6338 - categorical_accuracy: 0.8230 - top_3_accuracy: 0.9322 - val_loss: 0.7085 - val_categorical_accuracy: 0.8147 - val_top_3_accuracy: 0.9247\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.71557 to 0.70851, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 73/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.6285 - categorical_accuracy: 0.8248 - top_3_accuracy: 0.9331 - val_loss: 0.7207 - val_categorical_accuracy: 0.8118 - val_top_3_accuracy: 0.9238\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.70851\n",
      "Epoch 74/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.6305 - categorical_accuracy: 0.8233 - top_3_accuracy: 0.9325 - val_loss: 0.7148 - val_categorical_accuracy: 0.8138 - val_top_3_accuracy: 0.9256\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.70851\n",
      "Epoch 75/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.6262 - categorical_accuracy: 0.8245 - top_3_accuracy: 0.9332 - val_loss: 0.7183 - val_categorical_accuracy: 0.8122 - val_top_3_accuracy: 0.9244\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.70851\n",
      "Epoch 76/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6288 - categorical_accuracy: 0.8241 - top_3_accuracy: 0.9331 - val_loss: 0.7147 - val_categorical_accuracy: 0.8124 - val_top_3_accuracy: 0.9258\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.70851\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000/150000 [==============================] - 59s 395us/step - loss: 0.6231 - categorical_accuracy: 0.8251 - top_3_accuracy: 0.9331 - val_loss: 0.7145 - val_categorical_accuracy: 0.8132 - val_top_3_accuracy: 0.9252\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.70851\n",
      "Epoch 78/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6217 - categorical_accuracy: 0.8268 - top_3_accuracy: 0.9333 - val_loss: 0.7157 - val_categorical_accuracy: 0.8134 - val_top_3_accuracy: 0.9251\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.70851\n",
      "Epoch 79/100\n",
      "150000/150000 [==============================] - 59s 391us/step - loss: 0.6182 - categorical_accuracy: 0.8273 - top_3_accuracy: 0.9338 - val_loss: 0.7189 - val_categorical_accuracy: 0.8136 - val_top_3_accuracy: 0.9250\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.70851\n",
      "Epoch 80/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.6181 - categorical_accuracy: 0.8275 - top_3_accuracy: 0.9343 - val_loss: 0.7156 - val_categorical_accuracy: 0.8139 - val_top_3_accuracy: 0.9250\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.70851\n",
      "Epoch 81/100\n",
      "150000/150000 [==============================] - 59s 393us/step - loss: 0.6164 - categorical_accuracy: 0.8272 - top_3_accuracy: 0.9339 - val_loss: 0.7141 - val_categorical_accuracy: 0.8147 - val_top_3_accuracy: 0.9254\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.70851\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "Epoch 82/100\n",
      "150000/150000 [==============================] - 59s 392us/step - loss: 0.6152 - categorical_accuracy: 0.8271 - top_3_accuracy: 0.9347 - val_loss: 0.7131 - val_categorical_accuracy: 0.8140 - val_top_3_accuracy: 0.9254\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.70851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb24bbe9a58>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "stroke_read_model.fit(train_X, train_y,\n",
    "                      validation_data = (valid_X, valid_y), \n",
    "                      batch_size = batch_size,\n",
    "                      epochs = 100,\n",
    "                      callbacks = callbacks_list)\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7eb5b62-cf57-4380-8786-9ddc05be658f",
    "_uuid": "858059b6c16d81f86460bef8fcf595e0d68d12b2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stroke_read_model.load_weights(weight_path)\n",
    "lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 25000/25000 [==============================] - 3s 110us/step\n",
    "# Accuracy: 81.1%, Top 3 Accuracy 92.3%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
