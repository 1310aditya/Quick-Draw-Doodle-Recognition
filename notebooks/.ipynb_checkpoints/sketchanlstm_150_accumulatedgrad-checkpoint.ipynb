{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "8b08fbab2000a563b388f126eac74362641e497c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "STROKE_COUNT = 196\n",
    "TRAIN_SAMPLES = 4000\n",
    "VALID_SAMPLES = 1000\n",
    "TEST_SAMPLES = 1000\n",
    "NUM_CLASSES = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(69)\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from glob import glob\n",
    "import gc\n",
    "gc.enable()\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "base_dir = os.path.join('..')\n",
    "test_path = os.path.join(base_dir, 'test_simplified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7acacf8e960084782425ef1a1a3fd532a240ad48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "ALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\n",
    "COL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
    "\n",
    "def _stack_it(raw_strokes):\n",
    "    \"\"\"preprocess the string and make \n",
    "    a standard Nx3 stroke vector\"\"\"\n",
    "    stroke_vec = literal_eval(raw_strokes) # string->list\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,i)  \n",
    "     for i,(x,y) in enumerate(stroke_vec) \n",
    "     for xi,yi in zip(x,y)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "    # pad the strokes with zeros\n",
    "    x = pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "                         maxlen=STROKE_COUNT, \n",
    "                         padding='post').swapaxes(0, 1)\n",
    "    return x\n",
    "\n",
    "def read_batch(samples=5, \n",
    "               start_row=0,\n",
    "               max_rows = 1000):\n",
    "    \"\"\"\n",
    "    load and process the csv files\n",
    "    this function is horribly inefficient but simple\n",
    "    \"\"\"\n",
    "    out_df_list = []\n",
    "    for c_path in ALL_TRAIN_PATHS[:NUM_CLASSES]:\n",
    "        c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n",
    "        c_df.columns=COL_NAMES\n",
    "        out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n",
    "    full_df = pd.concat(out_df_list)\n",
    "    full_df['drawing'] = full_df['drawing'].\\\n",
    "        map(_stack_it)\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ec1854b21b36cd2fd7f7d0717aaa8da32506a6a"
   },
   "source": [
    "# Reading and Parsing\n",
    "Since it is too much data (23GB) to read in at once, we just take a portion of it for training, validation and hold-out testing. This should give us an idea about how well the model works, but leaves lots of room for improvement later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words 150 => The Great Wall of China, The Mona Lisa, angel, anvil, axe, banana, bandage, barn, basket, beach, bench, binoculars, bird, blackberry, bread, bridge, broccoli, broom, bush, butterfly, calculator, calendar, camel, camera, campfire, cannon, canoe, car, carrot, cat, ceiling fan, chair, circle, cloud, coffee cup, compass, cookie, cooler, cow, crocodile, cup, diamond, diving board, dolphin, door, dragon, drill, drums, duck, ear, elbow, eraser, face, fan, feather, finger, flashlight, flip flops, flying saucer, frog, garden, golf club, grass, hamburger, harp, hat, helmet, hexagon, hockey puck, hockey stick, hot air balloon, house, house plant, ice cream, jacket, ladder, light bulb, lighthouse, lion, lipstick, marker, microphone, moon, mosquito, mouse, mouth, mug, necklace, octagon, owl, paper clip, peanut, pear, peas, pencil, pickup truck, picture frame, pillow, pizza, police car, pond, popsicle, power outlet, purse, radio, rhinoceros, roller coaster, sailboat, sandwich, saxophone, scissors, screwdriver, shark, sheep, shorts, shovel, sink, skull, sleeping bag, snail, snorkel, snowflake, square, stereo, streetlight, submarine, suitcase, swan, sweater, swing set, teapot, telephone, toaster, tooth, toothpaste, tornado, tractor, train, tree, triangle, trombone, trumpet, vase, violin, washing machine, watermelon, wine bottle, wristwatch, yoga, zigzag\n"
     ]
    }
   ],
   "source": [
    "train_args = dict(samples=TRAIN_SAMPLES, \n",
    "                  start_row=0, \n",
    "                  max_rows=int(TRAIN_SAMPLES*1.5))\n",
    "valid_args = dict(samples=VALID_SAMPLES, \n",
    "                  start_row=train_args['max_rows']+1, \n",
    "                  max_rows=VALID_SAMPLES+25)\n",
    "test_args = dict(samples=TEST_SAMPLES, \n",
    "                 start_row=valid_args['max_rows']+train_args['max_rows']+1, \n",
    "                 max_rows=TEST_SAMPLES+25)\n",
    "train_df = read_batch(**train_args)\n",
    "valid_df = read_batch(**valid_args)\n",
    "test_df = read_batch(**test_args)\n",
    "word_encoder = LabelEncoder()\n",
    "word_encoder.fit(train_df['word'])\n",
    "print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d29237e-ece3-4dfd-9095-475296f4a608",
    "_uuid": "8bae16a4973a215861fbb536a602c4f5abf3b4bf"
   },
   "source": [
    "# Stroke-based Classification\n",
    "Here we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "ff5ddced-d77e-473f-899d-82cf11ad2bd9",
    "_uuid": "409468f1d5abd17b819482473a4f354a61f8d7ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600000, 196, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_Xy(in_df):\n",
    "    X = np.stack(in_df['drawing'], 0)\n",
    "    y = to_categorical(word_encoder.transform(in_df['word'].values))\n",
    "    return X, y\n",
    "train_X, train_y = get_Xy(train_df)\n",
    "valid_X, valid_y = get_Xy(valid_df)\n",
    "test_X, test_y = get_Xy(test_df)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "56240ed9-42b0-4f62-b3d1-f92017f04e30",
    "_uuid": "5cc79204a0a1da048d1d58ba8dfdafd0af3ebcb8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
    "# rand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\n",
    "# for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
    "#     test_arr = train_X[c_id]\n",
    "#     test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "#     lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "#     for i in np.unique(lab_idx):\n",
    "#         c_ax.plot(test_arr[lab_idx==i,0], \n",
    "#                 np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
    "#     c_ax.axis('off')\n",
    "#     c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e1d5bba-0fb4-432c-bd0b-ad69be0ef9ac",
    "_uuid": "b4a087a17798c2ec8eb520bc916bcad38d4ebff2",
    "collapsed": true
   },
   "source": [
    "# LSTM to Parse Strokes\n",
    "The model suggeted from the tutorial is\n",
    "\n",
    "![Suggested Model](https://www.tensorflow.org/versions/master/images/quickdraw_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "0ba4954b685952ec5a8b658245dfeacf7f42766c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout, MaxPool1D\n",
    "from keras.optimizers import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Optimizer\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "class Adam_accumulate(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=1e-8, accum_iters=20, **kwargs):\n",
    "        super(Adam_accumulate, self).__init__(**kwargs)\n",
    "        self.__dict__.update(locals())\n",
    "        self.iterations = K.variable(0)\n",
    "        self.lr = K.variable(lr)\n",
    "        self.beta_1 = K.variable(beta_1)\n",
    "        self.beta_2 = K.variable(beta_2)\n",
    "        self.accum_iters = K.variable(accum_iters)\n",
    "        \n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [(self.iterations, self.iterations + 1)]\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = self.lr * K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t))\n",
    "\n",
    "        ms = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n",
    "        vs = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n",
    "        gs = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n",
    "        self.weights = ms + vs\n",
    "\n",
    "        for p, g, m, v, gg in zip(params, grads, ms, vs, gs):\n",
    "\n",
    "            flag = K.equal(self.iterations % self.accum_iters, 0)\n",
    "            flag = K.cast(flag, dtype='float32')\n",
    "\n",
    "            gg_t = (1 - flag) * (gg + g)\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * (gg + flag * g) / self.accum_iters\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square((gg + flag * g) / self.accum_iters)\n",
    "            p_t = p - flag * lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append((m, flag * m_t + (1 - flag) * m))\n",
    "            self.updates.append((v, flag * v_t + (1 - flag) * v))\n",
    "            self.updates.append((gg, gg_t))\n",
    "\n",
    "            new_p = p_t\n",
    "            # apply constraints\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                c = constraints[p]\n",
    "                new_p = c(new_p)\n",
    "            self.updates.append((p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(Adam_accumulate, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "d65490e7-302e-4232-afe7-4e9499010e31",
    "_uuid": "ba9d55554ba9e4177df5f0645ca1e0f5e4393ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_10 (Batc (None, None, 3)           12        \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, None, 128)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, None, 256)         98560     \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_19 (CuDNNLSTM)    (None, None, 128)         197632    \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_20 (CuDNNLSTM)    (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 150)               76950     \n",
      "=================================================================\n",
      "Total params: 967,074\n",
      "Trainable params: 967,068\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if len(get_available_gpus())>0:\n",
    "    # https://twitter.com/fchollet/status/918170264608817152?lang=en\n",
    "    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances\n",
    "stroke_read_model = Sequential()\n",
    "stroke_read_model.add(BatchNormalization(input_shape = (None,)+train_X.shape[2:]))\n",
    "# filter count and length are taken from the script https://github.com/tensorflow/models/blob/master/tutorials/rnn/quickdraw/train_model.py\n",
    "stroke_read_model.add(Conv1D(128, (5,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(MaxPool1D(pool_size=3, strides=2))\n",
    "stroke_read_model.add(Conv1D(256, (3,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(Conv1D(256, (3,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(Conv1D(256, (3,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(MaxPool1D(pool_size=3, strides=2))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = True))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = False))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Dense(512))\n",
    "stroke_read_model.add(Dropout(0.4))\n",
    "stroke_read_model.add(Dense(len(word_encoder.classes_), activation = 'softmax'))\n",
    "#adam = optimizers.Adam(lr=0.0001)\n",
    "adam_acc = Adam_accumulate(accum_iters=3)\n",
    "stroke_read_model.compile(optimizer = adam_acc, # Combines updates for 5 iterations.\n",
    "                          loss = 'categorical_crossentropy', \n",
    "                          metrics = ['categorical_accuracy', top_3_accuracy])\n",
    "stroke_read_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "2a549512-a9d9-4afd-b748-3e1c3296e193",
    "_uuid": "5fda10b30c47a8cf6ea822ed0a4a1d7cd2c81195",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=3, \n",
    "                                   verbose=1, mode='auto', cooldown=3, min_lr=0.000005)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=10) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "84635ce7b1fffd7126ed82711ae52d9d57a35a24",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import Callback\n",
    "# class OutputClearNEpoch(Callback):\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         current = logs.get(self.monitor)\n",
    "#         if epoch % 5 == 0:\n",
    "#             clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "825b3af8-9451-487b-a1e1-538f2f1489e1",
    "_uuid": "ed2fc26af74aed1a93bbc253d61b72db5a81f5cc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600000 samples, validate on 150000 samples\n",
      "Epoch 1/150\n",
      "600000/600000 [==============================] - 177s 295us/step - loss: 4.6981 - categorical_accuracy: 0.0225 - top_3_accuracy: 0.0607 - val_loss: 4.6074 - val_categorical_accuracy: 0.0282 - val_top_3_accuracy: 0.0766\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.60736, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 2/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 4.5184 - categorical_accuracy: 0.0360 - top_3_accuracy: 0.0933 - val_loss: 4.2678 - val_categorical_accuracy: 0.0652 - val_top_3_accuracy: 0.1526\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.60736 to 4.26777, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 3/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 4.2191 - categorical_accuracy: 0.0666 - top_3_accuracy: 0.1588 - val_loss: 3.9815 - val_categorical_accuracy: 0.0984 - val_top_3_accuracy: 0.2190\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.26777 to 3.98150, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 4/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 3.7710 - categorical_accuracy: 0.1277 - top_3_accuracy: 0.2746 - val_loss: 3.4307 - val_categorical_accuracy: 0.1934 - val_top_3_accuracy: 0.3710\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.98150 to 3.43071, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 5/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 3.2140 - categorical_accuracy: 0.2248 - top_3_accuracy: 0.4233 - val_loss: 2.7835 - val_categorical_accuracy: 0.3197 - val_top_3_accuracy: 0.5358\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.43071 to 2.78346, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 6/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 2.7426 - categorical_accuracy: 0.3234 - top_3_accuracy: 0.5454 - val_loss: 2.3862 - val_categorical_accuracy: 0.4048 - val_top_3_accuracy: 0.6293\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.78346 to 2.38624, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 7/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 2.3850 - categorical_accuracy: 0.4038 - top_3_accuracy: 0.6300 - val_loss: 2.0339 - val_categorical_accuracy: 0.4855 - val_top_3_accuracy: 0.7042\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.38624 to 2.03389, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 8/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 2.1307 - categorical_accuracy: 0.4642 - top_3_accuracy: 0.6859 - val_loss: 1.8434 - val_categorical_accuracy: 0.5303 - val_top_3_accuracy: 0.7423\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.03389 to 1.84336, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 9/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.9629 - categorical_accuracy: 0.5045 - top_3_accuracy: 0.7203 - val_loss: 1.7215 - val_categorical_accuracy: 0.5565 - val_top_3_accuracy: 0.7659\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.84336 to 1.72152, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 10/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.8328 - categorical_accuracy: 0.5357 - top_3_accuracy: 0.7452 - val_loss: 1.6114 - val_categorical_accuracy: 0.5874 - val_top_3_accuracy: 0.7853\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.72152 to 1.61143, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 11/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.7369 - categorical_accuracy: 0.5592 - top_3_accuracy: 0.7635 - val_loss: 1.5266 - val_categorical_accuracy: 0.6087 - val_top_3_accuracy: 0.8005\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.61143 to 1.52663, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 12/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.6580 - categorical_accuracy: 0.5783 - top_3_accuracy: 0.7779 - val_loss: 1.4530 - val_categorical_accuracy: 0.6252 - val_top_3_accuracy: 0.8130\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.52663 to 1.45297, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 13/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.5869 - categorical_accuracy: 0.5956 - top_3_accuracy: 0.7907 - val_loss: 1.4377 - val_categorical_accuracy: 0.6283 - val_top_3_accuracy: 0.8170\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.45297 to 1.43767, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 14/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.5325 - categorical_accuracy: 0.6093 - top_3_accuracy: 0.8003 - val_loss: 1.3497 - val_categorical_accuracy: 0.6516 - val_top_3_accuracy: 0.8311\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.43767 to 1.34967, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 15/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.4823 - categorical_accuracy: 0.6213 - top_3_accuracy: 0.8091 - val_loss: 1.3176 - val_categorical_accuracy: 0.6588 - val_top_3_accuracy: 0.8367\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.34967 to 1.31764, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 16/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.4381 - categorical_accuracy: 0.6322 - top_3_accuracy: 0.8167 - val_loss: 1.2693 - val_categorical_accuracy: 0.6719 - val_top_3_accuracy: 0.8454\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.31764 to 1.26931, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 17/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.4034 - categorical_accuracy: 0.6405 - top_3_accuracy: 0.8231 - val_loss: 1.2213 - val_categorical_accuracy: 0.6841 - val_top_3_accuracy: 0.8529\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.26931 to 1.22126, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 18/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.3747 - categorical_accuracy: 0.6480 - top_3_accuracy: 0.8277 - val_loss: 1.2299 - val_categorical_accuracy: 0.6803 - val_top_3_accuracy: 0.8497\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.22126\n",
      "Epoch 19/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.3463 - categorical_accuracy: 0.6548 - top_3_accuracy: 0.8321 - val_loss: 1.1931 - val_categorical_accuracy: 0.6915 - val_top_3_accuracy: 0.8571\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.22126 to 1.19314, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 20/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.3188 - categorical_accuracy: 0.6614 - top_3_accuracy: 0.8365 - val_loss: 1.1648 - val_categorical_accuracy: 0.6975 - val_top_3_accuracy: 0.8611\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.19314 to 1.16481, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 21/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.2931 - categorical_accuracy: 0.6680 - top_3_accuracy: 0.8410 - val_loss: 1.1695 - val_categorical_accuracy: 0.6961 - val_top_3_accuracy: 0.8605\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.16481\n",
      "Epoch 22/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.2761 - categorical_accuracy: 0.6722 - top_3_accuracy: 0.8434 - val_loss: 1.1437 - val_categorical_accuracy: 0.7019 - val_top_3_accuracy: 0.8640\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.16481 to 1.14368, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 23/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.2548 - categorical_accuracy: 0.6775 - top_3_accuracy: 0.8472 - val_loss: 1.1150 - val_categorical_accuracy: 0.7116 - val_top_3_accuracy: 0.8684\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.14368 to 1.11495, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 24/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.2353 - categorical_accuracy: 0.6823 - top_3_accuracy: 0.8505 - val_loss: 1.1071 - val_categorical_accuracy: 0.7118 - val_top_3_accuracy: 0.8693\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.11495 to 1.10709, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 25/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.2177 - categorical_accuracy: 0.6872 - top_3_accuracy: 0.8530 - val_loss: 1.0950 - val_categorical_accuracy: 0.7146 - val_top_3_accuracy: 0.8722\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.10709 to 1.09505, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 26/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.2035 - categorical_accuracy: 0.6901 - top_3_accuracy: 0.8555 - val_loss: 1.0832 - val_categorical_accuracy: 0.7178 - val_top_3_accuracy: 0.8740\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.09505 to 1.08323, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 27/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.1891 - categorical_accuracy: 0.6937 - top_3_accuracy: 0.8576 - val_loss: 1.0640 - val_categorical_accuracy: 0.7222 - val_top_3_accuracy: 0.8759\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.08323 to 1.06398, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 28/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.1723 - categorical_accuracy: 0.6980 - top_3_accuracy: 0.8604 - val_loss: 1.0549 - val_categorical_accuracy: 0.7250 - val_top_3_accuracy: 0.8777\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.06398 to 1.05486, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 29/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.1572 - categorical_accuracy: 0.7016 - top_3_accuracy: 0.8631 - val_loss: 1.0478 - val_categorical_accuracy: 0.7265 - val_top_3_accuracy: 0.8784\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.05486 to 1.04781, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 30/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.1465 - categorical_accuracy: 0.7040 - top_3_accuracy: 0.8641 - val_loss: 1.0432 - val_categorical_accuracy: 0.7280 - val_top_3_accuracy: 0.8794\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.04781 to 1.04319, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 31/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.1359 - categorical_accuracy: 0.7068 - top_3_accuracy: 0.8661 - val_loss: 1.0344 - val_categorical_accuracy: 0.7305 - val_top_3_accuracy: 0.8800\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.04319 to 1.03445, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 32/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 1.1254 - categorical_accuracy: 0.7092 - top_3_accuracy: 0.8676 - val_loss: 1.0282 - val_categorical_accuracy: 0.7322 - val_top_3_accuracy: 0.8813\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.03445 to 1.02821, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 33/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.1166 - categorical_accuracy: 0.7119 - top_3_accuracy: 0.8689 - val_loss: 1.0186 - val_categorical_accuracy: 0.7347 - val_top_3_accuracy: 0.8828\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.02821 to 1.01858, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 34/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.1050 - categorical_accuracy: 0.7141 - top_3_accuracy: 0.8707 - val_loss: 1.0097 - val_categorical_accuracy: 0.7352 - val_top_3_accuracy: 0.8843\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.01858 to 1.00967, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 35/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0984 - categorical_accuracy: 0.7165 - top_3_accuracy: 0.8720 - val_loss: 0.9993 - val_categorical_accuracy: 0.7387 - val_top_3_accuracy: 0.8857\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.00967 to 0.99934, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 36/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0866 - categorical_accuracy: 0.7194 - top_3_accuracy: 0.8736 - val_loss: 0.9957 - val_categorical_accuracy: 0.7393 - val_top_3_accuracy: 0.8860\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.99934 to 0.99571, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 37/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0768 - categorical_accuracy: 0.7218 - top_3_accuracy: 0.8751 - val_loss: 0.9796 - val_categorical_accuracy: 0.7438 - val_top_3_accuracy: 0.8885\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.99571 to 0.97961, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 38/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0690 - categorical_accuracy: 0.7236 - top_3_accuracy: 0.8763 - val_loss: 0.9795 - val_categorical_accuracy: 0.7444 - val_top_3_accuracy: 0.8887\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.97961 to 0.97948, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 39/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0614 - categorical_accuracy: 0.7257 - top_3_accuracy: 0.8774 - val_loss: 0.9787 - val_categorical_accuracy: 0.7436 - val_top_3_accuracy: 0.8887\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.97948 to 0.97872, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 40/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0510 - categorical_accuracy: 0.7276 - top_3_accuracy: 0.8792 - val_loss: 0.9738 - val_categorical_accuracy: 0.7455 - val_top_3_accuracy: 0.8892\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.97872 to 0.97378, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 41/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0443 - categorical_accuracy: 0.7300 - top_3_accuracy: 0.8796 - val_loss: 0.9607 - val_categorical_accuracy: 0.7500 - val_top_3_accuracy: 0.8921\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.97378 to 0.96067, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 42/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0375 - categorical_accuracy: 0.7316 - top_3_accuracy: 0.8810 - val_loss: 0.9623 - val_categorical_accuracy: 0.7484 - val_top_3_accuracy: 0.8911\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.96067\n",
      "Epoch 43/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0268 - categorical_accuracy: 0.7340 - top_3_accuracy: 0.8822 - val_loss: 0.9540 - val_categorical_accuracy: 0.7515 - val_top_3_accuracy: 0.8919\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.96067 to 0.95404, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 44/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0258 - categorical_accuracy: 0.7342 - top_3_accuracy: 0.8827 - val_loss: 0.9474 - val_categorical_accuracy: 0.7528 - val_top_3_accuracy: 0.8933\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.95404 to 0.94740, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 45/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0167 - categorical_accuracy: 0.7364 - top_3_accuracy: 0.8842 - val_loss: 0.9383 - val_categorical_accuracy: 0.7552 - val_top_3_accuracy: 0.8946\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.94740 to 0.93830, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 46/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0121 - categorical_accuracy: 0.7372 - top_3_accuracy: 0.8845 - val_loss: 0.9323 - val_categorical_accuracy: 0.7559 - val_top_3_accuracy: 0.8953\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.93830 to 0.93228, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 47/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0062 - categorical_accuracy: 0.7394 - top_3_accuracy: 0.8858 - val_loss: 0.9348 - val_categorical_accuracy: 0.7569 - val_top_3_accuracy: 0.8949\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.93228\n",
      "Epoch 48/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 1.0006 - categorical_accuracy: 0.7404 - top_3_accuracy: 0.8862 - val_loss: 0.9218 - val_categorical_accuracy: 0.7597 - val_top_3_accuracy: 0.8973\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.93228 to 0.92180, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 49/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9967 - categorical_accuracy: 0.7416 - top_3_accuracy: 0.8873 - val_loss: 0.9264 - val_categorical_accuracy: 0.7578 - val_top_3_accuracy: 0.8961\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.92180\n",
      "Epoch 50/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9880 - categorical_accuracy: 0.7438 - top_3_accuracy: 0.8884 - val_loss: 0.9277 - val_categorical_accuracy: 0.7589 - val_top_3_accuracy: 0.8953\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.92180\n",
      "Epoch 51/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9837 - categorical_accuracy: 0.7440 - top_3_accuracy: 0.8888 - val_loss: 0.9203 - val_categorical_accuracy: 0.7595 - val_top_3_accuracy: 0.8970\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.92180 to 0.92031, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 52/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9793 - categorical_accuracy: 0.7457 - top_3_accuracy: 0.8897 - val_loss: 0.9220 - val_categorical_accuracy: 0.7606 - val_top_3_accuracy: 0.8974\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.92031\n",
      "Epoch 53/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9730 - categorical_accuracy: 0.7472 - top_3_accuracy: 0.8903 - val_loss: 0.9177 - val_categorical_accuracy: 0.7609 - val_top_3_accuracy: 0.8970\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.92031 to 0.91766, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 54/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9685 - categorical_accuracy: 0.7483 - top_3_accuracy: 0.8911 - val_loss: 0.9203 - val_categorical_accuracy: 0.7600 - val_top_3_accuracy: 0.8977\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.91766\n",
      "Epoch 55/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9657 - categorical_accuracy: 0.7489 - top_3_accuracy: 0.8918 - val_loss: 0.9132 - val_categorical_accuracy: 0.7620 - val_top_3_accuracy: 0.8981\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.91766 to 0.91316, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 56/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9585 - categorical_accuracy: 0.7506 - top_3_accuracy: 0.8925 - val_loss: 0.9113 - val_categorical_accuracy: 0.7621 - val_top_3_accuracy: 0.8982\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.91316 to 0.91134, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 57/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9554 - categorical_accuracy: 0.7521 - top_3_accuracy: 0.8934 - val_loss: 0.8991 - val_categorical_accuracy: 0.7654 - val_top_3_accuracy: 0.9001\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.91134 to 0.89905, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 58/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9527 - categorical_accuracy: 0.7523 - top_3_accuracy: 0.8931 - val_loss: 0.8969 - val_categorical_accuracy: 0.7663 - val_top_3_accuracy: 0.9006\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.89905 to 0.89692, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 59/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9461 - categorical_accuracy: 0.7542 - top_3_accuracy: 0.8939 - val_loss: 0.8944 - val_categorical_accuracy: 0.7664 - val_top_3_accuracy: 0.9005\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.89692 to 0.89439, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 60/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9427 - categorical_accuracy: 0.7548 - top_3_accuracy: 0.8946 - val_loss: 0.9004 - val_categorical_accuracy: 0.7657 - val_top_3_accuracy: 0.9000\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.89439\n",
      "Epoch 61/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9400 - categorical_accuracy: 0.7553 - top_3_accuracy: 0.8956 - val_loss: 0.9124 - val_categorical_accuracy: 0.7625 - val_top_3_accuracy: 0.8984\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.89439\n",
      "Epoch 62/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9335 - categorical_accuracy: 0.7570 - top_3_accuracy: 0.8963 - val_loss: 0.8920 - val_categorical_accuracy: 0.7669 - val_top_3_accuracy: 0.9013\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.89439 to 0.89202, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 63/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.9316 - categorical_accuracy: 0.7576 - top_3_accuracy: 0.8970 - val_loss: 0.8881 - val_categorical_accuracy: 0.7680 - val_top_3_accuracy: 0.9020\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.89202 to 0.88811, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 64/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9276 - categorical_accuracy: 0.7587 - top_3_accuracy: 0.8970 - val_loss: 0.8976 - val_categorical_accuracy: 0.7665 - val_top_3_accuracy: 0.9007\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.88811\n",
      "Epoch 65/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.9216 - categorical_accuracy: 0.7597 - top_3_accuracy: 0.8976 - val_loss: 0.8763 - val_categorical_accuracy: 0.7718 - val_top_3_accuracy: 0.9036\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.88811 to 0.87629, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 66/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.9177 - categorical_accuracy: 0.7608 - top_3_accuracy: 0.8984 - val_loss: 0.8894 - val_categorical_accuracy: 0.7674 - val_top_3_accuracy: 0.9020\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.87629\n",
      "Epoch 67/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.9147 - categorical_accuracy: 0.7618 - top_3_accuracy: 0.8984 - val_loss: 0.8810 - val_categorical_accuracy: 0.7699 - val_top_3_accuracy: 0.9023\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.87629\n",
      "Epoch 68/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.9098 - categorical_accuracy: 0.7629 - top_3_accuracy: 0.8993 - val_loss: 0.8784 - val_categorical_accuracy: 0.7712 - val_top_3_accuracy: 0.9030\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.87629\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Epoch 69/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.8878 - categorical_accuracy: 0.7685 - top_3_accuracy: 0.9026 - val_loss: 0.8608 - val_categorical_accuracy: 0.7762 - val_top_3_accuracy: 0.9055\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.87629 to 0.86082, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 70/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8833 - categorical_accuracy: 0.7691 - top_3_accuracy: 0.9032 - val_loss: 0.8610 - val_categorical_accuracy: 0.7757 - val_top_3_accuracy: 0.9059\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.86082\n",
      "Epoch 71/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8804 - categorical_accuracy: 0.7704 - top_3_accuracy: 0.9035 - val_loss: 0.8616 - val_categorical_accuracy: 0.7748 - val_top_3_accuracy: 0.9056\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.86082\n",
      "Epoch 72/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8754 - categorical_accuracy: 0.7710 - top_3_accuracy: 0.9041 - val_loss: 0.8631 - val_categorical_accuracy: 0.7756 - val_top_3_accuracy: 0.9056\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.86082\n",
      "Epoch 73/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8761 - categorical_accuracy: 0.7709 - top_3_accuracy: 0.9040 - val_loss: 0.8601 - val_categorical_accuracy: 0.7753 - val_top_3_accuracy: 0.9057\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.86082 to 0.86005, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 74/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8743 - categorical_accuracy: 0.7713 - top_3_accuracy: 0.9047 - val_loss: 0.8665 - val_categorical_accuracy: 0.7752 - val_top_3_accuracy: 0.9051\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.86005\n",
      "Epoch 75/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8736 - categorical_accuracy: 0.7719 - top_3_accuracy: 0.9043 - val_loss: 0.8599 - val_categorical_accuracy: 0.7760 - val_top_3_accuracy: 0.9058\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.86005 to 0.85988, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 76/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8712 - categorical_accuracy: 0.7726 - top_3_accuracy: 0.9049 - val_loss: 0.8627 - val_categorical_accuracy: 0.7754 - val_top_3_accuracy: 0.9059\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.85988\n",
      "Epoch 77/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8664 - categorical_accuracy: 0.7742 - top_3_accuracy: 0.9057 - val_loss: 0.8571 - val_categorical_accuracy: 0.7768 - val_top_3_accuracy: 0.9063\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.85988 to 0.85713, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 78/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8646 - categorical_accuracy: 0.7741 - top_3_accuracy: 0.9061 - val_loss: 0.8563 - val_categorical_accuracy: 0.7778 - val_top_3_accuracy: 0.9068\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.85713 to 0.85627, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 79/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8660 - categorical_accuracy: 0.7739 - top_3_accuracy: 0.9054 - val_loss: 0.8573 - val_categorical_accuracy: 0.7766 - val_top_3_accuracy: 0.9068\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.85627\n",
      "Epoch 80/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8623 - categorical_accuracy: 0.7742 - top_3_accuracy: 0.9064 - val_loss: 0.8598 - val_categorical_accuracy: 0.7762 - val_top_3_accuracy: 0.9066\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.85627\n",
      "Epoch 81/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8600 - categorical_accuracy: 0.7753 - top_3_accuracy: 0.9064 - val_loss: 0.8503 - val_categorical_accuracy: 0.7791 - val_top_3_accuracy: 0.9081\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.85627 to 0.85033, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 82/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.8571 - categorical_accuracy: 0.7757 - top_3_accuracy: 0.9068 - val_loss: 0.8541 - val_categorical_accuracy: 0.7787 - val_top_3_accuracy: 0.9074\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.85033\n",
      "Epoch 83/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8568 - categorical_accuracy: 0.7759 - top_3_accuracy: 0.9070 - val_loss: 0.8515 - val_categorical_accuracy: 0.7784 - val_top_3_accuracy: 0.9079\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.85033\n",
      "Epoch 84/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8551 - categorical_accuracy: 0.7761 - top_3_accuracy: 0.9072 - val_loss: 0.8569 - val_categorical_accuracy: 0.7769 - val_top_3_accuracy: 0.9073\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.85033\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "Epoch 85/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8408 - categorical_accuracy: 0.7803 - top_3_accuracy: 0.9087 - val_loss: 0.8431 - val_categorical_accuracy: 0.7802 - val_top_3_accuracy: 0.9088\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.85033 to 0.84313, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 86/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.8345 - categorical_accuracy: 0.7814 - top_3_accuracy: 0.9096 - val_loss: 0.8516 - val_categorical_accuracy: 0.7788 - val_top_3_accuracy: 0.9076\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.84313\n",
      "Epoch 87/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.8346 - categorical_accuracy: 0.7810 - top_3_accuracy: 0.9097 - val_loss: 0.8436 - val_categorical_accuracy: 0.7810 - val_top_3_accuracy: 0.9087\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.84313\n",
      "Epoch 88/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8322 - categorical_accuracy: 0.7816 - top_3_accuracy: 0.9101 - val_loss: 0.8488 - val_categorical_accuracy: 0.7794 - val_top_3_accuracy: 0.9083\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.84313\n",
      "Epoch 89/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8315 - categorical_accuracy: 0.7820 - top_3_accuracy: 0.9105 - val_loss: 0.8479 - val_categorical_accuracy: 0.7802 - val_top_3_accuracy: 0.9081\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.84313\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "Epoch 90/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.8221 - categorical_accuracy: 0.7847 - top_3_accuracy: 0.9115 - val_loss: 0.8379 - val_categorical_accuracy: 0.7832 - val_top_3_accuracy: 0.9096\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.84313 to 0.83793, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 91/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.8191 - categorical_accuracy: 0.7853 - top_3_accuracy: 0.9120 - val_loss: 0.8362 - val_categorical_accuracy: 0.7828 - val_top_3_accuracy: 0.9096\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.83793 to 0.83621, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 92/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.8198 - categorical_accuracy: 0.7852 - top_3_accuracy: 0.9115 - val_loss: 0.8400 - val_categorical_accuracy: 0.7822 - val_top_3_accuracy: 0.9094\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.83621\n",
      "Epoch 93/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8177 - categorical_accuracy: 0.7854 - top_3_accuracy: 0.9119 - val_loss: 0.8380 - val_categorical_accuracy: 0.7828 - val_top_3_accuracy: 0.9094\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.83621\n",
      "Epoch 94/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8184 - categorical_accuracy: 0.7851 - top_3_accuracy: 0.9118 - val_loss: 0.8350 - val_categorical_accuracy: 0.7834 - val_top_3_accuracy: 0.9096\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.83621 to 0.83502, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 95/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8182 - categorical_accuracy: 0.7852 - top_3_accuracy: 0.9121 - val_loss: 0.8393 - val_categorical_accuracy: 0.7825 - val_top_3_accuracy: 0.9093\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.83502\n",
      "Epoch 96/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8161 - categorical_accuracy: 0.7857 - top_3_accuracy: 0.9123 - val_loss: 0.8352 - val_categorical_accuracy: 0.7833 - val_top_3_accuracy: 0.9097\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.83502\n",
      "Epoch 97/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8174 - categorical_accuracy: 0.7852 - top_3_accuracy: 0.9125 - val_loss: 0.8349 - val_categorical_accuracy: 0.7838 - val_top_3_accuracy: 0.9095\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.83502 to 0.83492, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "Epoch 98/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8107 - categorical_accuracy: 0.7870 - top_3_accuracy: 0.9130 - val_loss: 0.8343 - val_categorical_accuracy: 0.7837 - val_top_3_accuracy: 0.9100\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.83492 to 0.83428, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 99/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.8071 - categorical_accuracy: 0.7881 - top_3_accuracy: 0.9135 - val_loss: 0.8306 - val_categorical_accuracy: 0.7848 - val_top_3_accuracy: 0.9100\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.83428 to 0.83056, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 100/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8074 - categorical_accuracy: 0.7880 - top_3_accuracy: 0.9134 - val_loss: 0.8365 - val_categorical_accuracy: 0.7827 - val_top_3_accuracy: 0.9098\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.83056\n",
      "Epoch 101/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8078 - categorical_accuracy: 0.7883 - top_3_accuracy: 0.9133 - val_loss: 0.8322 - val_categorical_accuracy: 0.7839 - val_top_3_accuracy: 0.9106\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.83056\n",
      "Epoch 102/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8063 - categorical_accuracy: 0.7881 - top_3_accuracy: 0.9135 - val_loss: 0.8336 - val_categorical_accuracy: 0.7836 - val_top_3_accuracy: 0.9100\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.83056\n",
      "\n",
      "Epoch 00102: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "Epoch 103/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.8019 - categorical_accuracy: 0.7892 - top_3_accuracy: 0.9141 - val_loss: 0.8304 - val_categorical_accuracy: 0.7846 - val_top_3_accuracy: 0.9104\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.83056 to 0.83038, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 104/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8033 - categorical_accuracy: 0.7887 - top_3_accuracy: 0.9142 - val_loss: 0.8342 - val_categorical_accuracy: 0.7839 - val_top_3_accuracy: 0.9099\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.83038\n",
      "Epoch 105/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8017 - categorical_accuracy: 0.7889 - top_3_accuracy: 0.9142 - val_loss: 0.8312 - val_categorical_accuracy: 0.7849 - val_top_3_accuracy: 0.9105\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.83038\n",
      "Epoch 106/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8005 - categorical_accuracy: 0.7895 - top_3_accuracy: 0.9142 - val_loss: 0.8299 - val_categorical_accuracy: 0.7847 - val_top_3_accuracy: 0.9108\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.83038 to 0.82987, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 107/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8000 - categorical_accuracy: 0.7897 - top_3_accuracy: 0.9144 - val_loss: 0.8297 - val_categorical_accuracy: 0.7851 - val_top_3_accuracy: 0.9109\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.82987 to 0.82972, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 108/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.8006 - categorical_accuracy: 0.7899 - top_3_accuracy: 0.9143 - val_loss: 0.8288 - val_categorical_accuracy: 0.7853 - val_top_3_accuracy: 0.9109\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.82972 to 0.82883, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 109/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.7991 - categorical_accuracy: 0.7899 - top_3_accuracy: 0.9146 - val_loss: 0.8323 - val_categorical_accuracy: 0.7850 - val_top_3_accuracy: 0.9101\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.82883\n",
      "Epoch 110/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.7977 - categorical_accuracy: 0.7902 - top_3_accuracy: 0.9145 - val_loss: 0.8309 - val_categorical_accuracy: 0.7852 - val_top_3_accuracy: 0.9104\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.82883\n",
      "Epoch 111/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.7988 - categorical_accuracy: 0.7900 - top_3_accuracy: 0.9142 - val_loss: 0.8302 - val_categorical_accuracy: 0.7852 - val_top_3_accuracy: 0.9104\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.82883\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "Epoch 112/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.7960 - categorical_accuracy: 0.7908 - top_3_accuracy: 0.9150 - val_loss: 0.8292 - val_categorical_accuracy: 0.7853 - val_top_3_accuracy: 0.9108\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.82883\n",
      "Epoch 113/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.7962 - categorical_accuracy: 0.7909 - top_3_accuracy: 0.9149 - val_loss: 0.8292 - val_categorical_accuracy: 0.7855 - val_top_3_accuracy: 0.9107\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.82883\n",
      "Epoch 114/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.7965 - categorical_accuracy: 0.7902 - top_3_accuracy: 0.9149 - val_loss: 0.8308 - val_categorical_accuracy: 0.7848 - val_top_3_accuracy: 0.9105\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.82883\n",
      "Epoch 115/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7950 - categorical_accuracy: 0.7912 - top_3_accuracy: 0.9150 - val_loss: 0.8270 - val_categorical_accuracy: 0.7858 - val_top_3_accuracy: 0.9108\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.82883 to 0.82699, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 116/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7956 - categorical_accuracy: 0.7915 - top_3_accuracy: 0.9148 - val_loss: 0.8287 - val_categorical_accuracy: 0.7856 - val_top_3_accuracy: 0.9108\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.82699\n",
      "Epoch 117/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7953 - categorical_accuracy: 0.7912 - top_3_accuracy: 0.9150 - val_loss: 0.8299 - val_categorical_accuracy: 0.7855 - val_top_3_accuracy: 0.9107\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.82699\n",
      "Epoch 118/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7963 - categorical_accuracy: 0.7905 - top_3_accuracy: 0.9150 - val_loss: 0.8308 - val_categorical_accuracy: 0.7848 - val_top_3_accuracy: 0.9104\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.82699\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "Epoch 119/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7947 - categorical_accuracy: 0.7912 - top_3_accuracy: 0.9150 - val_loss: 0.8286 - val_categorical_accuracy: 0.7854 - val_top_3_accuracy: 0.9106\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.82699\n",
      "Epoch 120/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7920 - categorical_accuracy: 0.7914 - top_3_accuracy: 0.9153 - val_loss: 0.8302 - val_categorical_accuracy: 0.7850 - val_top_3_accuracy: 0.9106\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.82699\n",
      "Epoch 121/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7936 - categorical_accuracy: 0.7913 - top_3_accuracy: 0.9152 - val_loss: 0.8287 - val_categorical_accuracy: 0.7855 - val_top_3_accuracy: 0.9107\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.82699\n",
      "Epoch 122/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.7944 - categorical_accuracy: 0.7911 - top_3_accuracy: 0.9153 - val_loss: 0.8269 - val_categorical_accuracy: 0.7860 - val_top_3_accuracy: 0.9109\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.82699 to 0.82691, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 123/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7952 - categorical_accuracy: 0.7908 - top_3_accuracy: 0.9148 - val_loss: 0.8291 - val_categorical_accuracy: 0.7853 - val_top_3_accuracy: 0.9107\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.82691\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "Epoch 124/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7922 - categorical_accuracy: 0.7913 - top_3_accuracy: 0.9156 - val_loss: 0.8282 - val_categorical_accuracy: 0.7856 - val_top_3_accuracy: 0.9107\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.82691\n",
      "Epoch 125/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7916 - categorical_accuracy: 0.7922 - top_3_accuracy: 0.9156 - val_loss: 0.8276 - val_categorical_accuracy: 0.7857 - val_top_3_accuracy: 0.9109\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.82691\n",
      "Epoch 126/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7921 - categorical_accuracy: 0.7920 - top_3_accuracy: 0.9154 - val_loss: 0.8285 - val_categorical_accuracy: 0.7856 - val_top_3_accuracy: 0.9107\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.82691\n",
      "Epoch 127/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7923 - categorical_accuracy: 0.7916 - top_3_accuracy: 0.9152 - val_loss: 0.8288 - val_categorical_accuracy: 0.7855 - val_top_3_accuracy: 0.9106\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.82691\n",
      "Epoch 128/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7912 - categorical_accuracy: 0.7916 - top_3_accuracy: 0.9153 - val_loss: 0.8275 - val_categorical_accuracy: 0.7857 - val_top_3_accuracy: 0.9109\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.82691\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "Epoch 129/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7910 - categorical_accuracy: 0.7920 - top_3_accuracy: 0.9154 - val_loss: 0.8279 - val_categorical_accuracy: 0.7857 - val_top_3_accuracy: 0.9110\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.82691\n",
      "Epoch 130/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7921 - categorical_accuracy: 0.7918 - top_3_accuracy: 0.9154 - val_loss: 0.8287 - val_categorical_accuracy: 0.7855 - val_top_3_accuracy: 0.9108\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.82691\n",
      "Epoch 131/150\n",
      "600000/600000 [==============================] - 172s 287us/step - loss: 0.7912 - categorical_accuracy: 0.7924 - top_3_accuracy: 0.9156 - val_loss: 0.8285 - val_categorical_accuracy: 0.7855 - val_top_3_accuracy: 0.9109\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.82691\n",
      "Epoch 132/150\n",
      "600000/600000 [==============================] - 173s 288us/step - loss: 0.7916 - categorical_accuracy: 0.7914 - top_3_accuracy: 0.9151 - val_loss: 0.8272 - val_categorical_accuracy: 0.7859 - val_top_3_accuracy: 0.9110\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.82691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fde77360278>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "stroke_read_model.fit(train_X, train_y,\n",
    "                      validation_data = (valid_X, valid_y), \n",
    "                      batch_size = batch_size,\n",
    "                      epochs = 150,\n",
    "                      callbacks = callbacks_list)\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "a7eb5b62-cf57-4380-8786-9ddc05be658f",
    "_uuid": "858059b6c16d81f86460bef8fcf595e0d68d12b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000/150000 [==============================] - 11s 73us/step\n",
      "Accuracy: 78.6%, Top 3 Accuracy 91.2%\n"
     ]
    }
   ],
   "source": [
    "stroke_read_model.load_weights(weight_path)\n",
    "lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
