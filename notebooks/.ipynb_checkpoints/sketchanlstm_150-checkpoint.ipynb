{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "522ce330-d4f9-4fbe-9684-4b65fd684cca",
    "_uuid": "174484daa5f084ce4970f5048d3e05d2c4429787"
   },
   "source": [
    "# Overview\n",
    "The notebook is modified from one that was made for the [Quick, Draw Dataset](https://www.kaggle.com/google/tinyquickdraw), it would actually be interesting to see how beneficial a transfer learning approach using that data as a starting point could be.\n",
    "\n",
    "## This Notebook\n",
    "The notebook takes and preprocesses the data from the QuickDraw Competition step (strokes) and trains an LSTM. The outcome variable (y) is always the same (category). The stroke-based LSTM. The model takes the stroke data and 'preprocesses' it a bit using 1D convolutions and then uses two stacked LSTMs followed by two dense layers to make the classification. The model can be thought to 'read' the drawing stroke by stroke.\n",
    "\n",
    "## Fun Models\n",
    "\n",
    "After the classification models, we try to build a few models to understand what the LSTM actually does. Here we experiment step by step to see how the prediction changes with each stop\n",
    "\n",
    "### Next Steps\n",
    "The next steps could be\n",
    "- use more data to train\n",
    "- include the country code (different countries draw different things, different ways)\n",
    "- more complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8ccded02a5f2c4a9d9ee2f7688114bcd2e1f11a"
   },
   "source": [
    "### Model Parameters\n",
    "Here we keep track of the relevant parameters for the data preprocessing, model construction and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "8b08fbab2000a563b388f126eac74362641e497c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "STROKE_COUNT = 196\n",
    "TRAIN_SAMPLES = 1750\n",
    "VALID_SAMPLES = 300\n",
    "TEST_SAMPLES = 300\n",
    "NUM_CLASSES = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(69)\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from glob import glob\n",
    "import gc\n",
    "gc.enable()\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "base_dir = os.path.join('..', 'input')\n",
    "test_path = os.path.join(base_dir, 'test_simplified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "7acacf8e960084782425ef1a1a3fd532a240ad48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "ALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\n",
    "COL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
    "\n",
    "def _stack_it(raw_strokes):\n",
    "    \"\"\"preprocess the string and make \n",
    "    a standard Nx3 stroke vector\"\"\"\n",
    "    stroke_vec = literal_eval(raw_strokes) # string->list\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,i)  \n",
    "     for i,(x,y) in enumerate(stroke_vec) \n",
    "     for xi,yi in zip(x,y)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "    # pad the strokes with zeros\n",
    "    x = pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "                         maxlen=STROKE_COUNT, \n",
    "                         padding='post').swapaxes(0, 1)\n",
    "    return x\n",
    "\n",
    "def read_batch(samples=5, \n",
    "               start_row=0,\n",
    "               max_rows = 1000):\n",
    "    \"\"\"\n",
    "    load and process the csv files\n",
    "    this function is horribly inefficient but simple\n",
    "    \"\"\"\n",
    "    out_df_list = []\n",
    "    for c_path in ALL_TRAIN_PATHS[:NUM_CLASSES]:\n",
    "        c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n",
    "        c_df.columns=COL_NAMES\n",
    "        out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n",
    "    full_df = pd.concat(out_df_list)\n",
    "    full_df['drawing'] = full_df['drawing'].\\\n",
    "        map(_stack_it)\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ec1854b21b36cd2fd7f7d0717aaa8da32506a6a"
   },
   "source": [
    "# Reading and Parsing\n",
    "Since it is too much data (23GB) to read in at once, we just take a portion of it for training, validation and hold-out testing. This should give us an idea about how well the model works, but leaves lots of room for improvement later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words 150 => The Great Wall of China, alarm clock, angel, animal migration, ant, anvil, apple, asparagus, axe, banana, barn, baseball, bat, bathtub, beach, bear, bicycle, birthday cake, blackberry, blueberry, book, boomerang, bowtie, bread, bridge, broccoli, bus, bush, butterfly, cake, calendar, campfire, canoe, cat, ceiling fan, cell phone, chandelier, church, circle, clarinet, computer, cookie, crown, cup, diving board, dog, dolphin, donut, door, dresser, drill, elbow, elephant, eyeglasses, face, fan, fireplace, flip flops, foot, garden, golf club, grapes, grass, hamburger, hammer, headphones, helicopter, hockey puck, hospital, hot air balloon, hot dog, hourglass, house, house plant, hurricane, ice cream, jacket, jail, key, lantern, map, marker, megaphone, mermaid, monkey, mosquito, motorbike, mountain, mushroom, nose, ocean, octopus, onion, paper clip, parrot, passport, peanut, pineapple, pliers, police car, pool, popsicle, potato, purse, rake, river, roller coaster, sandwich, sea turtle, see saw, shark, sink, skateboard, sleeping bag, snail, soccer ball, sock, spider, spoon, spreadsheet, square, squiggle, squirrel, stereo, stethoscope, stove, suitcase, swing set, t-shirt, television, tent, toe, tooth, toothbrush, toothpaste, traffic light, tree, triangle, trumpet, van, violin, washing machine, watermelon, waterslide, wheel, windmill, wine glass, yoga, zebra, zigzag\n"
     ]
    }
   ],
   "source": [
    "train_args = dict(samples=TRAIN_SAMPLES, \n",
    "                  start_row=0, \n",
    "                  max_rows=int(TRAIN_SAMPLES*1.5))\n",
    "valid_args = dict(samples=VALID_SAMPLES, \n",
    "                  start_row=train_args['max_rows']+1, \n",
    "                  max_rows=VALID_SAMPLES+25)\n",
    "test_args = dict(samples=TEST_SAMPLES, \n",
    "                 start_row=valid_args['max_rows']+train_args['max_rows']+1, \n",
    "                 max_rows=TEST_SAMPLES+25)\n",
    "train_df = read_batch(**train_args)\n",
    "valid_df = read_batch(**valid_args)\n",
    "test_df = read_batch(**test_args)\n",
    "word_encoder = LabelEncoder()\n",
    "word_encoder.fit(train_df['word'])\n",
    "print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d29237e-ece3-4dfd-9095-475296f4a608",
    "_uuid": "8bae16a4973a215861fbb536a602c4f5abf3b4bf"
   },
   "source": [
    "# Stroke-based Classification\n",
    "Here we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_cell_guid": "ff5ddced-d77e-473f-899d-82cf11ad2bd9",
    "_uuid": "409468f1d5abd17b819482473a4f354a61f8d7ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262500, 196, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_Xy(in_df):\n",
    "    X = np.stack(in_df['drawing'], 0)\n",
    "    y = to_categorical(word_encoder.transform(in_df['word'].values))\n",
    "    return X, y\n",
    "train_X, train_y = get_Xy(train_df)\n",
    "valid_X, valid_y = get_Xy(valid_df)\n",
    "test_X, test_y = get_Xy(test_df)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_cell_guid": "56240ed9-42b0-4f62-b3d1-f92017f04e30",
    "_uuid": "5cc79204a0a1da048d1d58ba8dfdafd0af3ebcb8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
    "# rand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\n",
    "# for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
    "#     test_arr = train_X[c_id]\n",
    "#     test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "#     lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "#     for i in np.unique(lab_idx):\n",
    "#         c_ax.plot(test_arr[lab_idx==i,0], \n",
    "#                 np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
    "#     c_ax.axis('off')\n",
    "#     c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e1d5bba-0fb4-432c-bd0b-ad69be0ef9ac",
    "_uuid": "b4a087a17798c2ec8eb520bc916bcad38d4ebff2",
    "collapsed": true
   },
   "source": [
    "# LSTM to Parse Strokes\n",
    "The model suggeted from the tutorial is\n",
    "\n",
    "![Suggested Model](https://www.tensorflow.org/versions/master/images/quickdraw_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_uuid": "0ba4954b685952ec5a8b658245dfeacf7f42766c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout, MaxPool1D\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_cell_guid": "d65490e7-302e-4232-afe7-4e9499010e31",
    "_uuid": "ba9d55554ba9e4177df5f0645ca1e0f5e4393ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_11 (Batc (None, None, 3)           12        \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, None, 128)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, None, 256)         98560     \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_17 (CuDNNLSTM)    (None, None, 128)         197632    \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_18 (CuDNNLSTM)    (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 150)               76950     \n",
      "=================================================================\n",
      "Total params: 967,074\n",
      "Trainable params: 967,068\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if len(get_available_gpus())>0:\n",
    "    # https://twitter.com/fchollet/status/918170264608817152?lang=en\n",
    "    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances\n",
    "stroke_read_model = Sequential()\n",
    "stroke_read_model.add(BatchNormalization(input_shape = (None,)+train_X.shape[2:]))\n",
    "# filter count and length are taken from the script https://github.com/tensorflow/models/blob/master/tutorials/rnn/quickdraw/train_model.py\n",
    "stroke_read_model.add(Conv1D(128, (5,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(MaxPool1D(pool_size=3, strides=2))\n",
    "stroke_read_model.add(Conv1D(256, (3,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(Conv1D(256, (3,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(Conv1D(256, (3,), padding='same', activation='relu'))\n",
    "stroke_read_model.add(Dropout(0.15))\n",
    "stroke_read_model.add(MaxPool1D(pool_size=3, strides=2))\n",
    "stroke_read_model.add(Dropout(0.2))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = True))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = False))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Dense(512))\n",
    "stroke_read_model.add(Dropout(0.4))\n",
    "stroke_read_model.add(Dense(len(word_encoder.classes_), activation = 'softmax'))\n",
    "adam = optimizers.Adam(lr=0.0001)\n",
    "stroke_read_model.compile(optimizer = adam, \n",
    "                          loss = 'categorical_crossentropy', \n",
    "                          metrics = ['categorical_accuracy', top_3_accuracy])\n",
    "stroke_read_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "_cell_guid": "2a549512-a9d9-4afd-b748-3e1c3296e193",
    "_uuid": "5fda10b30c47a8cf6ea822ed0a4a1d7cd2c81195",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=3, \n",
    "                                   verbose=1, mode='auto', cooldown=3, min_lr=0.000005)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=10) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "_uuid": "84635ce7b1fffd7126ed82711ae52d9d57a35a24",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import Callback\n",
    "# class OutputClearNEpoch(Callback):\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         current = logs.get(self.monitor)\n",
    "#         if epoch % 5 == 0:\n",
    "#             clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_cell_guid": "825b3af8-9451-487b-a1e1-538f2f1489e1",
    "_uuid": "ed2fc26af74aed1a93bbc253d61b72db5a81f5cc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150000 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "150000/150000 [==============================] - 67s 443us/step - loss: 3.6198 - categorical_accuracy: 0.0569 - top_3_accuracy: 0.1486 - val_loss: 3.5977 - val_categorical_accuracy: 0.0591 - val_top_3_accuracy: 0.1548\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.59772, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 2/100\n",
      "150000/150000 [==============================] - 62s 416us/step - loss: 3.4732 - categorical_accuracy: 0.0806 - top_3_accuracy: 0.1969 - val_loss: 3.3677 - val_categorical_accuracy: 0.1053 - val_top_3_accuracy: 0.2431\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.59772 to 3.36770, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 3/100\n",
      "150000/150000 [==============================] - 62s 415us/step - loss: 3.2430 - categorical_accuracy: 0.1250 - top_3_accuracy: 0.2812 - val_loss: 3.0787 - val_categorical_accuracy: 0.1580 - val_top_3_accuracy: 0.3368\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.36770 to 3.07866, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 4/100\n",
      "150000/150000 [==============================] - 62s 416us/step - loss: 2.9742 - categorical_accuracy: 0.1813 - top_3_accuracy: 0.3846 - val_loss: 2.6559 - val_categorical_accuracy: 0.2572 - val_top_3_accuracy: 0.4930\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.07866 to 2.65586, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 5/100\n",
      "150000/150000 [==============================] - 62s 415us/step - loss: 2.5719 - categorical_accuracy: 0.2739 - top_3_accuracy: 0.5208 - val_loss: 2.3352 - val_categorical_accuracy: 0.3307 - val_top_3_accuracy: 0.5873\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.65586 to 2.33522, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 6/100\n",
      "150000/150000 [==============================] - 62s 415us/step - loss: 2.2646 - categorical_accuracy: 0.3564 - top_3_accuracy: 0.6129 - val_loss: 2.0269 - val_categorical_accuracy: 0.4165 - val_top_3_accuracy: 0.6713\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.33522 to 2.02689, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 7/100\n",
      "150000/150000 [==============================] - 62s 415us/step - loss: 1.9872 - categorical_accuracy: 0.4372 - top_3_accuracy: 0.6873 - val_loss: 1.8493 - val_categorical_accuracy: 0.4747 - val_top_3_accuracy: 0.7124\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.02689 to 1.84932, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 8/100\n",
      "150000/150000 [==============================] - 62s 415us/step - loss: 1.7443 - categorical_accuracy: 0.5067 - top_3_accuracy: 0.7452 - val_loss: 1.5446 - val_categorical_accuracy: 0.5602 - val_top_3_accuracy: 0.7860\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.84932 to 1.54464, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 9/100\n",
      "150000/150000 [==============================] - 62s 415us/step - loss: 1.5615 - categorical_accuracy: 0.5601 - top_3_accuracy: 0.7855 - val_loss: 1.3842 - val_categorical_accuracy: 0.6110 - val_top_3_accuracy: 0.8205\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.54464 to 1.38419, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 10/100\n",
      "150000/150000 [==============================] - 62s 415us/step - loss: 1.4287 - categorical_accuracy: 0.5987 - top_3_accuracy: 0.8101 - val_loss: 1.2800 - val_categorical_accuracy: 0.6406 - val_top_3_accuracy: 0.8402\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.38419 to 1.27999, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 11/100\n",
      "150000/150000 [==============================] - 62s 414us/step - loss: 1.3257 - categorical_accuracy: 0.6307 - top_3_accuracy: 0.8293 - val_loss: 1.1738 - val_categorical_accuracy: 0.6710 - val_top_3_accuracy: 0.8558\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.27999 to 1.17376, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 12/100\n",
      "150000/150000 [==============================] - 62s 415us/step - loss: 1.2486 - categorical_accuracy: 0.6519 - top_3_accuracy: 0.8428 - val_loss: 1.1565 - val_categorical_accuracy: 0.6780 - val_top_3_accuracy: 0.8552\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.17376 to 1.15645, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 13/100\n",
      "150000/150000 [==============================] - 62s 414us/step - loss: 1.1858 - categorical_accuracy: 0.6704 - top_3_accuracy: 0.8531 - val_loss: 1.1109 - val_categorical_accuracy: 0.6883 - val_top_3_accuracy: 0.8626\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.15645 to 1.11086, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 14/100\n",
      "150000/150000 [==============================] - 62s 414us/step - loss: 1.1244 - categorical_accuracy: 0.6869 - top_3_accuracy: 0.8623 - val_loss: 1.0665 - val_categorical_accuracy: 0.7000 - val_top_3_accuracy: 0.8712\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.11086 to 1.06654, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 15/100\n",
      "150000/150000 [==============================] - 62s 415us/step - loss: 1.0823 - categorical_accuracy: 0.7004 - top_3_accuracy: 0.8697 - val_loss: 1.0375 - val_categorical_accuracy: 0.7114 - val_top_3_accuracy: 0.8725\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.06654 to 1.03746, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 16/100\n",
      "150000/150000 [==============================] - 62s 415us/step - loss: 1.0494 - categorical_accuracy: 0.7101 - top_3_accuracy: 0.8739 - val_loss: 0.9692 - val_categorical_accuracy: 0.7292 - val_top_3_accuracy: 0.8870\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.03746 to 0.96920, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 17/100\n",
      "150000/150000 [==============================] - 62s 414us/step - loss: 1.0074 - categorical_accuracy: 0.7215 - top_3_accuracy: 0.8795 - val_loss: 0.9357 - val_categorical_accuracy: 0.7427 - val_top_3_accuracy: 0.8915\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.96920 to 0.93569, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 18/100\n",
      "150000/150000 [==============================] - 62s 414us/step - loss: 0.9707 - categorical_accuracy: 0.7312 - top_3_accuracy: 0.8850 - val_loss: 0.8943 - val_categorical_accuracy: 0.7523 - val_top_3_accuracy: 0.8969\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.93569 to 0.89431, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 19/100\n",
      "150000/150000 [==============================] - 62s 414us/step - loss: 0.9414 - categorical_accuracy: 0.7399 - top_3_accuracy: 0.8900 - val_loss: 0.9086 - val_categorical_accuracy: 0.7466 - val_top_3_accuracy: 0.8958\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.89431\n",
      "Epoch 20/100\n",
      "150000/150000 [==============================] - 62s 414us/step - loss: 0.9164 - categorical_accuracy: 0.7469 - top_3_accuracy: 0.8937 - val_loss: 0.8787 - val_categorical_accuracy: 0.7529 - val_top_3_accuracy: 0.9016\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.89431 to 0.87868, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 21/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.8857 - categorical_accuracy: 0.7558 - top_3_accuracy: 0.8974 - val_loss: 0.8444 - val_categorical_accuracy: 0.7658 - val_top_3_accuracy: 0.9039\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.87868 to 0.84439, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 22/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.8733 - categorical_accuracy: 0.7585 - top_3_accuracy: 0.8996 - val_loss: 0.8367 - val_categorical_accuracy: 0.7700 - val_top_3_accuracy: 0.9067\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.84439 to 0.83674, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 23/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.8446 - categorical_accuracy: 0.7667 - top_3_accuracy: 0.9032 - val_loss: 0.8184 - val_categorical_accuracy: 0.7728 - val_top_3_accuracy: 0.9083\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.83674 to 0.81841, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 24/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.8400 - categorical_accuracy: 0.7677 - top_3_accuracy: 0.9046 - val_loss: 0.8294 - val_categorical_accuracy: 0.7697 - val_top_3_accuracy: 0.9086\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.81841\n",
      "Epoch 25/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.8096 - categorical_accuracy: 0.7770 - top_3_accuracy: 0.9082 - val_loss: 0.8131 - val_categorical_accuracy: 0.7749 - val_top_3_accuracy: 0.9099\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.81841 to 0.81307, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 26/100\n",
      "150000/150000 [==============================] - 62s 414us/step - loss: 0.7955 - categorical_accuracy: 0.7802 - top_3_accuracy: 0.9104 - val_loss: 0.7793 - val_categorical_accuracy: 0.7862 - val_top_3_accuracy: 0.9145\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.81307 to 0.77935, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 27/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.7777 - categorical_accuracy: 0.7851 - top_3_accuracy: 0.9123 - val_loss: 0.8078 - val_categorical_accuracy: 0.7789 - val_top_3_accuracy: 0.9103\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.77935\n",
      "Epoch 28/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.7638 - categorical_accuracy: 0.7892 - top_3_accuracy: 0.9151 - val_loss: 0.7507 - val_categorical_accuracy: 0.7942 - val_top_3_accuracy: 0.9175\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.77935 to 0.75072, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 29/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.7504 - categorical_accuracy: 0.7927 - top_3_accuracy: 0.9165 - val_loss: 0.7454 - val_categorical_accuracy: 0.7939 - val_top_3_accuracy: 0.9190\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.75072 to 0.74545, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 30/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.7349 - categorical_accuracy: 0.7961 - top_3_accuracy: 0.9191 - val_loss: 0.7454 - val_categorical_accuracy: 0.7960 - val_top_3_accuracy: 0.9186\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.74545 to 0.74537, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 31/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.7244 - categorical_accuracy: 0.8000 - top_3_accuracy: 0.9197 - val_loss: 0.7231 - val_categorical_accuracy: 0.8027 - val_top_3_accuracy: 0.9203\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.74537 to 0.72308, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 32/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.7078 - categorical_accuracy: 0.8048 - top_3_accuracy: 0.9223 - val_loss: 0.7325 - val_categorical_accuracy: 0.7985 - val_top_3_accuracy: 0.9196\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.72308\n",
      "Epoch 33/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.6970 - categorical_accuracy: 0.8069 - top_3_accuracy: 0.9232 - val_loss: 0.7040 - val_categorical_accuracy: 0.8051 - val_top_3_accuracy: 0.9228\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.72308 to 0.70404, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 34/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.6868 - categorical_accuracy: 0.8100 - top_3_accuracy: 0.9253 - val_loss: 0.7004 - val_categorical_accuracy: 0.8080 - val_top_3_accuracy: 0.9226\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.70404 to 0.70039, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 35/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.6815 - categorical_accuracy: 0.8115 - top_3_accuracy: 0.9260 - val_loss: 0.7189 - val_categorical_accuracy: 0.8038 - val_top_3_accuracy: 0.9213\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.70039\n",
      "Epoch 36/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.6720 - categorical_accuracy: 0.8142 - top_3_accuracy: 0.9272 - val_loss: 0.7148 - val_categorical_accuracy: 0.8041 - val_top_3_accuracy: 0.9224\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.70039\n",
      "Epoch 37/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.6653 - categorical_accuracy: 0.8164 - top_3_accuracy: 0.9282 - val_loss: 0.6950 - val_categorical_accuracy: 0.8111 - val_top_3_accuracy: 0.9240\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.70039 to 0.69503, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 38/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.6489 - categorical_accuracy: 0.8204 - top_3_accuracy: 0.9296 - val_loss: 0.6866 - val_categorical_accuracy: 0.8134 - val_top_3_accuracy: 0.9252\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.69503 to 0.68664, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 39/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.6391 - categorical_accuracy: 0.8228 - top_3_accuracy: 0.9303 - val_loss: 0.6871 - val_categorical_accuracy: 0.8122 - val_top_3_accuracy: 0.9263\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.68664\n",
      "Epoch 40/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.6352 - categorical_accuracy: 0.8239 - top_3_accuracy: 0.9309 - val_loss: 0.6921 - val_categorical_accuracy: 0.8133 - val_top_3_accuracy: 0.9244\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.68664\n",
      "Epoch 41/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.6231 - categorical_accuracy: 0.8273 - top_3_accuracy: 0.9334 - val_loss: 0.6816 - val_categorical_accuracy: 0.8147 - val_top_3_accuracy: 0.9249\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.68664 to 0.68160, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 42/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.6100 - categorical_accuracy: 0.8314 - top_3_accuracy: 0.9350 - val_loss: 0.6658 - val_categorical_accuracy: 0.8189 - val_top_3_accuracy: 0.9285\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.68160 to 0.66583, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 43/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.6133 - categorical_accuracy: 0.8301 - top_3_accuracy: 0.9343 - val_loss: 0.6612 - val_categorical_accuracy: 0.8185 - val_top_3_accuracy: 0.9290\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.66583 to 0.66124, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 44/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.6068 - categorical_accuracy: 0.8317 - top_3_accuracy: 0.9347 - val_loss: 0.6808 - val_categorical_accuracy: 0.8181 - val_top_3_accuracy: 0.9267\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.66124\n",
      "Epoch 45/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.5921 - categorical_accuracy: 0.8354 - top_3_accuracy: 0.9365 - val_loss: 0.6759 - val_categorical_accuracy: 0.8157 - val_top_3_accuracy: 0.9269\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.66124\n",
      "Epoch 46/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.5871 - categorical_accuracy: 0.8369 - top_3_accuracy: 0.9383 - val_loss: 0.6688 - val_categorical_accuracy: 0.8189 - val_top_3_accuracy: 0.9280\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.66124\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Epoch 47/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.5545 - categorical_accuracy: 0.8460 - top_3_accuracy: 0.9412 - val_loss: 0.6488 - val_categorical_accuracy: 0.8253 - val_top_3_accuracy: 0.9310\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.66124 to 0.64881, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 48/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.5486 - categorical_accuracy: 0.8479 - top_3_accuracy: 0.9412 - val_loss: 0.6379 - val_categorical_accuracy: 0.8280 - val_top_3_accuracy: 0.9308\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.64881 to 0.63791, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 49/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.5426 - categorical_accuracy: 0.8490 - top_3_accuracy: 0.9431 - val_loss: 0.6400 - val_categorical_accuracy: 0.8287 - val_top_3_accuracy: 0.9305\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.63791\n",
      "Epoch 50/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.5387 - categorical_accuracy: 0.8502 - top_3_accuracy: 0.9426 - val_loss: 0.6344 - val_categorical_accuracy: 0.8290 - val_top_3_accuracy: 0.9315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: val_loss improved from 0.63791 to 0.63439, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 51/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.5352 - categorical_accuracy: 0.8512 - top_3_accuracy: 0.9432 - val_loss: 0.6433 - val_categorical_accuracy: 0.8280 - val_top_3_accuracy: 0.9318\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.63439\n",
      "Epoch 52/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.5270 - categorical_accuracy: 0.8527 - top_3_accuracy: 0.9448 - val_loss: 0.6447 - val_categorical_accuracy: 0.8276 - val_top_3_accuracy: 0.9308\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.63439\n",
      "Epoch 53/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.5269 - categorical_accuracy: 0.8535 - top_3_accuracy: 0.9452 - val_loss: 0.6402 - val_categorical_accuracy: 0.8299 - val_top_3_accuracy: 0.9313\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.63439\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "Epoch 54/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.5060 - categorical_accuracy: 0.8583 - top_3_accuracy: 0.9468 - val_loss: 0.6247 - val_categorical_accuracy: 0.8317 - val_top_3_accuracy: 0.9325\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.63439 to 0.62472, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 55/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4963 - categorical_accuracy: 0.8610 - top_3_accuracy: 0.9485 - val_loss: 0.6311 - val_categorical_accuracy: 0.8319 - val_top_3_accuracy: 0.9327\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.62472\n",
      "Epoch 56/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4942 - categorical_accuracy: 0.8617 - top_3_accuracy: 0.9480 - val_loss: 0.6458 - val_categorical_accuracy: 0.8276 - val_top_3_accuracy: 0.9317\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.62472\n",
      "Epoch 57/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4920 - categorical_accuracy: 0.8632 - top_3_accuracy: 0.9481 - val_loss: 0.6357 - val_categorical_accuracy: 0.8309 - val_top_3_accuracy: 0.9323\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.62472\n",
      "Epoch 58/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.4883 - categorical_accuracy: 0.8630 - top_3_accuracy: 0.9492 - val_loss: 0.6383 - val_categorical_accuracy: 0.8298 - val_top_3_accuracy: 0.9329\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.62472\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "Epoch 59/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4769 - categorical_accuracy: 0.8658 - top_3_accuracy: 0.9502 - val_loss: 0.6287 - val_categorical_accuracy: 0.8345 - val_top_3_accuracy: 0.9330\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.62472\n",
      "Epoch 60/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4724 - categorical_accuracy: 0.8679 - top_3_accuracy: 0.9508 - val_loss: 0.6270 - val_categorical_accuracy: 0.8339 - val_top_3_accuracy: 0.9337\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.62472\n",
      "Epoch 61/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.4700 - categorical_accuracy: 0.8683 - top_3_accuracy: 0.9510 - val_loss: 0.6227 - val_categorical_accuracy: 0.8342 - val_top_3_accuracy: 0.9337\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.62472 to 0.62268, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 62/100\n",
      "150000/150000 [==============================] - 62s 413us/step - loss: 0.4696 - categorical_accuracy: 0.8691 - top_3_accuracy: 0.9510 - val_loss: 0.6328 - val_categorical_accuracy: 0.8337 - val_top_3_accuracy: 0.9323\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.62268\n",
      "Epoch 63/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4673 - categorical_accuracy: 0.8691 - top_3_accuracy: 0.9511 - val_loss: 0.6300 - val_categorical_accuracy: 0.8350 - val_top_3_accuracy: 0.9328\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.62268\n",
      "Epoch 64/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4674 - categorical_accuracy: 0.8684 - top_3_accuracy: 0.9512 - val_loss: 0.6278 - val_categorical_accuracy: 0.8326 - val_top_3_accuracy: 0.9332\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.62268\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "Epoch 65/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4578 - categorical_accuracy: 0.8717 - top_3_accuracy: 0.9526 - val_loss: 0.6263 - val_categorical_accuracy: 0.8344 - val_top_3_accuracy: 0.9333\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.62268\n",
      "Epoch 66/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4534 - categorical_accuracy: 0.8733 - top_3_accuracy: 0.9533 - val_loss: 0.6277 - val_categorical_accuracy: 0.8338 - val_top_3_accuracy: 0.9335\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.62268\n",
      "Epoch 67/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4535 - categorical_accuracy: 0.8725 - top_3_accuracy: 0.9535 - val_loss: 0.6248 - val_categorical_accuracy: 0.8356 - val_top_3_accuracy: 0.9337\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.62268\n",
      "Epoch 68/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4512 - categorical_accuracy: 0.8733 - top_3_accuracy: 0.9534 - val_loss: 0.6301 - val_categorical_accuracy: 0.8340 - val_top_3_accuracy: 0.9335\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.62268\n",
      "Epoch 69/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4521 - categorical_accuracy: 0.8735 - top_3_accuracy: 0.9528 - val_loss: 0.6300 - val_categorical_accuracy: 0.8340 - val_top_3_accuracy: 0.9342\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.62268\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "Epoch 70/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4450 - categorical_accuracy: 0.8748 - top_3_accuracy: 0.9541 - val_loss: 0.6264 - val_categorical_accuracy: 0.8351 - val_top_3_accuracy: 0.9346\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.62268\n",
      "Epoch 71/100\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.4426 - categorical_accuracy: 0.8751 - top_3_accuracy: 0.9539 - val_loss: 0.6289 - val_categorical_accuracy: 0.8345 - val_top_3_accuracy: 0.9339\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.62268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2a9db4d68>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "stroke_read_model.fit(train_X, train_y,\n",
    "                      validation_data = (valid_X, valid_y), \n",
    "                      batch_size = batch_size,\n",
    "                      epochs = 150,\n",
    "                      callbacks = callbacks_list)\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "a7eb5b62-cf57-4380-8786-9ddc05be658f",
    "_uuid": "858059b6c16d81f86460bef8fcf595e0d68d12b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 4s 148us/step\n",
      "Accuracy: 83.6%, Top 3 Accuracy 93.3%\n"
     ]
    }
   ],
   "source": [
    "stroke_read_model.load_weights(weight_path)\n",
    "lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
